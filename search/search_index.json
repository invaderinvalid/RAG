{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Scientific Literature Explorer","text":"<p>A production-ready RAG system that retrieves scientific papers, compresses context, and generates well-cited answers through a multi-stage anti-hallucination pipeline.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Scientific Literature Explorer is an intelligent research assistant that: - Automatically discovers relevant papers from ArXiv - Uses TF-IDF-based RAG for precise chunk retrieval - Compresses context via ScaleDown API (40-60% token reduction) - Runs a multi-stage reasoning workflow (COT \u2192 Verify \u2192 Critique) - Enforces strict citation rules to minimize hallucination - Maintains session history for multi-turn conversations</p> <p>Built with Google Gemini 2.5 Flash for intelligence and ScaleDown API for context compression.</p>"},{"location":"#key-features","title":"Key Features","text":"Feature Description \ud83d\udd0d Smart Discovery Auto-discovers papers via ArXiv API with parallel downloads \ud83d\udcca Context Compression ScaleDown API reduces tokens by 40-60% while preserving meaning \ud83e\udde0 Multi-Stage Reasoning Chain-of-Thought \u2192 Self-Verification \u2192 Self-Critique \ud83d\udcdd Strict Citations Every claim requires an inline citation <code>[arxiv:XXXX.XXXXX]</code> \ud83d\udcac Session Persistence Multi-turn conversations with history context \u26a1 Question Triage General questions answered instantly without paper fetch \ud83c\udf9b\ufe0f Configurable Pipeline Toggle stages, reorder workflow via CLI \ud83d\udd04 Resilient Fallback Automatic retry with exponential backoff + ScaleDown fallback"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-install","title":"1. Install","text":"<pre><code>git clone &lt;repo-url&gt;\ncd RAG\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\npip install -r requirements.txt\n</code></pre>"},{"location":"#2-configure","title":"2. Configure","text":"<p>Create <code>.env</code> from <code>.env.example</code>:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Fill in your API keys:</p> <pre><code>SCALEDOWN_API_KEY=your_scaledown_api_key\nGEMINI_API_KEY=your_gemini_api_key\n</code></pre> <p>Get keys: - ScaleDown: ScaleDown Getting Started - Gemini: Google AI Studio (free tier available)</p>"},{"location":"#3-run","title":"3. Run","text":"<pre><code># Ask a research question\npython -m src.main ask \"What are the latest advances in neural architecture search?\"\n\n# Interactive paper explorer\npython -m src.main papers \"transformers attention mechanism\"\n\n# Deep-dive into a specific paper\npython -m src.main paper 1706.03762 \"What is multi-head attention?\"\n</code></pre>"},{"location":"#system-comparison","title":"System Comparison","text":"Feature This System Traditional RAG Paper Discovery \u2705 Automatic ArXiv search + parallel downloads \u274c Manual paper curation Context Compression \u2705 ScaleDown API (40-60% reduction) \u274c No compression (high token costs) Verification \u2705 Multi-stage: COT \u2192 Verify \u2192 Critique \u274c Single-pass generation Citations \u2705 Strict inline citations enforced \u26a0\ufe0f Optional, often missing Triage \u2705 Smart routing (general vs research) \u274c All queries treated equally Sessions \u2705 Persistent multi-turn conversations \u274c Stateless single-shot Fallback \u2705 ScaleDown fallback on rate limits \u274c Hard failure Rate Limit Handling \u2705 Exponential backoff (5\u00d7 retries) \u26a0\ufe0f Basic retry or none"},{"location":"#example-workflow","title":"Example Workflow","text":""},{"location":"#research-question","title":"Research Question","text":"<pre><code>$ python -m src.main ask \"What are transformers in NLP?\"\n</code></pre> <p>What happens: 1. \u26a1 Question triaged as \"research\" 2. \ud83d\udd0d ArXiv searched for relevant papers 3. \ud83d\udce5 PDFs downloaded in parallel 4. \u2702\ufe0f Text chunked and indexed (TF-IDF) 5. \ud83d\udcca Top-5 chunks compressed via ScaleDown (1500 \u2192 600 tokens) 6. \ud83e\udde0 COT reasoning with strict citations 7. \u2705 Self-verification checks all citations 8. \ud83d\udccb Self-critique evaluates quality 9. \ud83d\udcbe Session saved for follow-ups</p> <p>Result: A cited answer in ~45-60 seconds</p>"},{"location":"#follow-up-question","title":"Follow-Up Question","text":"<pre><code>$ python -m src.main ask \"How does this compare to RNNs?\" --session abc123\n</code></pre> <p>What happens: 1. \u26a1 Session loaded (previous papers + conversation history) 2. \ud83d\udcda No re-downloading (papers cached) 3. \ud83e\udde0 Full pipeline runs with context from previous Q&amp;A 4. \ud83d\udcbe Session updated</p> <p>Result: A contextual answer in ~20-30 seconds</p>"},{"location":"#interactive-paper-explorer","title":"Interactive Paper Explorer","text":"<pre><code>$ python -m src.main papers \"attention mechanism transformers\"\n</code></pre> <p>Features: - \ud83d\udccb Browse search results - \ud83c\udfaf Select a paper - \ud83d\udcac Ask questions about it - \ud83d\udd04 Switch between papers seamlessly - \ud83d\udcdd All questions share one session - \u26a1 Instant follow-ups (no refetching)</p> <p>Interactive commands: - Type text: Ask a question - Type number: Switch papers - Type <code>back</code>: Return to list - Type <code>s</code>: New search - Type <code>q</code>: Quit</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Architecture Overview \u2014 System components and data flow</li> <li>How It Works \u2014 End-to-end flow with ScaleDown and Gemini roles</li> <li>Setup Guide \u2014 Installation and configuration</li> <li>Configuration \u2014 All environment variables explained</li> </ul>"},{"location":"#usage","title":"Usage","text":"<ul> <li>Usage Guide \u2014 All CLI commands and examples</li> <li>Workflow Examples \u2014 Common usage patterns</li> </ul>"},{"location":"#technical-details","title":"Technical Details","text":"<ul> <li>Methodology \u2014 RAG, compression, triage, resilience strategies</li> <li>Anti-Hallucination Pipeline \u2014 Multi-stage verification details</li> <li>API Reference \u2014 Complete command and config reference</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Project Structure \u2014 Codebase organization and file descriptions</li> <li>Limitations \u2014 Known constraints and trade-offs</li> <li>Improvements \u2014 Future enhancements (short/medium/long-term)</li> </ul>"},{"location":"#technology-stack","title":"Technology Stack","text":"Layer Technology Purpose Intelligence Google Gemini 2.5 Flash Answer generation, classification, verification Compression ScaleDown API Context compression (40-60%), fallback generation Paper Source ArXiv Atom API Scientific paper search and metadata PDF Processing PyPDF2 Text extraction from PDFs Retrieval scikit-learn TF-IDF Vectorization and similarity search Storage JSON (sessions), Markdown (artifacts) Persistence CLI Rich (terminal UI) Interactive tables, panels, markdown rendering"},{"location":"#performance","title":"Performance","text":""},{"location":"#latency","title":"Latency","text":"Query Type Time Breakdown General Question ~5-7s Triage (2s) + Direct Answer (5s) Research Question (first) ~45-60s Discovery (15s) + Extraction (5s) + Pipeline (30s) Follow-Up ~20-30s Cached papers + Pipeline (20s)"},{"location":"#token-efficiency","title":"Token Efficiency","text":"<p>Without ScaleDown: - Retrieved context: ~1500 tokens - API cost: Higher - Latency: Slower</p> <p>With ScaleDown: - Compressed context: ~600 tokens (40% reduction) - API cost: 40% lower - Latency: 20% faster (less to process)</p>"},{"location":"#project-status","title":"Project Status","text":"<p>Production-Ready Features: - \u2705 Multi-paper discovery - \u2705 Context compression - \u2705 Multi-stage verification - \u2705 Session persistence - \u2705 Interactive paper explorer - \u2705 Configurable workflow - \u2705 Rate limit resilience</p> <p>Known Limitations: - ArXiv-only (no IEEE, ACM, PubMed) - TF-IDF retrieval (not semantic) - No streaming responses - CLI only (no web UI)</p> <p>See Limitations for details.</p>"},{"location":"#contributing-improvements","title":"Contributing &amp; Improvements","text":"<p>See Improvements for a roadmap of potential enhancements:</p> <p>Short-term wins: - Semantic embeddings (better retrieval) - Async API calls (lower latency) - ScaleDown Python SDK (cleaner code)</p> <p>Long-term goals: - Multi-source support (Semantic Scholar, PubMed) - Knowledge graph for cross-paper reasoning - Web UI (Streamlit/Gradio)</p>"},{"location":"#license","title":"License","text":"<p>The original project specification can be found in the root <code>Project.md</code> file.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>\ud83d\udc49 New users: Start with Getting Started</p> <p>\ud83d\udc49 Want to understand the system: Read Architecture and How It Works</p> <p>\ud83d\udc49 Ready to use: Jump to Usage Guide</p> <p>\ud83d\udc49 Technical deep-dive: Explore Methodology and Anti-Hallucination Pipeline</p>"},{"location":"anti-hallucination/","title":"Anti-Hallucination Pipeline","text":"<p>The system uses a multi-stage approach to minimize hallucination and ensure factual accuracy.</p>"},{"location":"anti-hallucination/#pipeline-overview","title":"Pipeline Overview","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#1976d2','primaryTextColor':'#fff','primaryBorderColor':'#0d47a1','lineColor':'#424242','secondaryColor':'#f57c00','tertiaryColor':'#2e7d32'}}}%%\nsequenceDiagram\n    participant User\n    participant RAG as RAG Pipeline\n    participant SD as ScaleDown\n    participant G1 as Gemini COT\n    participant G2 as Gemini Verify\n    participant G3 as Gemini Critique\n    participant Store as Artifact Store\n\n    User-&gt;&gt;RAG: Research Question\n    RAG-&gt;&gt;RAG: Retrieve top-k chunks (TF-IDF)\n    RAG-&gt;&gt;SD: Compress context ~1500 tokens\n    SD--&gt;&gt;RAG: Compressed ~600 tokens (40%)\n\n    RAG-&gt;&gt;G1: System: Strict citation rules&lt;br/&gt;User: Question + Context\n    Note over G1: thinking_budget: 2048&lt;br/&gt;max_tokens: 8192\n    G1-&gt;&gt;G1: Generate COT with inline citations\n    G1--&gt;&gt;Store: COT Answer [arxiv:XXXX] tags\n\n    Store-&gt;&gt;G2: Verify citations Draft + Sources\n    Note over G2: thinking_budget: 1024&lt;br/&gt;max_tokens: 4096&lt;br/&gt;(Faster/Cheaper)\n    G2-&gt;&gt;G2: Check each citation against source text\n    G2--&gt;&gt;Store: Verification Table SUPPORTED/NOT FOUND\n\n    Store-&gt;&gt;G3: Critique answer for completeness\n    Note over G3: thinking_budget: 1024&lt;br/&gt;max_tokens: 4096\n    G3-&gt;&gt;G3: Evaluate quality suggest improvements\n    G3--&gt;&gt;Store: Critique Report\n\n    Store-&gt;&gt;SD: Compress artifacts\n    SD--&gt;&gt;Store: Compressed markdown\n    Store--&gt;&gt;User: Final Answer + Citation Summary</code></pre>"},{"location":"anti-hallucination/#stage-1-strict-citation-rules-cot","title":"Stage 1: Strict Citation Rules (COT)","text":"<p>The COT handler enforces mandatory citation rules in its system prompt:</p> <pre><code>CITATION RULES (mandatory):\n- Every factual claim MUST have an inline citation like [arxiv:XXXX.XXXXX]\n- Quote or closely paraphrase the source text\n- If a claim has no supporting source, mark it as [unsupported \u2014 general knowledge]\n- End with a ## References section listing all cited sources\n</code></pre>"},{"location":"anti-hallucination/#goal","title":"Goal","text":"<p>Force the LLM to: - Ground every claim in source documents - Make citation tracking explicit - Distinguish between paper facts and general knowledge</p>"},{"location":"anti-hallucination/#output-format","title":"Output Format","text":"<p>The COT stage produces: - Step-by-step reasoning - Inline citations for every claim - A final <code>## References</code> section with all sources</p> <p>Example:</p> <pre><code>## Chain-of-Thought Analysis\n\nTransformers use self-attention mechanisms to process input sequences\n[arxiv:1706.03762]. Unlike RNNs, they allow parallel processing of all\ntokens simultaneously [arxiv:1706.03762].\n\nThe multi-head attention mechanism splits the input into multiple\nrepresentation subspaces, allowing the model to attend to information\nfrom different positions [arxiv:1706.03762].\n\n## References\n\n- [arxiv:1706.03762] Vaswani et al., \"Attention Is All You Need\"\n</code></pre>"},{"location":"anti-hallucination/#stage-2-self-verification","title":"Stage 2: Self-Verification","text":"<p>The verify handler checks every citation:</p> <pre><code>For EACH citation:\n1. Find the exact passage in Source Context that supports it\n2. Quote that passage verbatim\n3. Rate: SUPPORTED / PARTIALLY SUPPORTED / NOT FOUND IN SOURCES\n4. Flag any claims that SHOULD have a citation but don't\n\nOutput: Markdown table \u2192 Claim | Citation | Source Quote | Verdict\n</code></pre>"},{"location":"anti-hallucination/#goal_1","title":"Goal","text":"<p>Cross-check every claim against source documents: - Catch hallucinations (claims with no source) - Validate citation accuracy (correct paper cited) - Identify missing citations (claims that need sources)</p>"},{"location":"anti-hallucination/#output-format_1","title":"Output Format","text":"<p>The verify stage produces a markdown table:</p> Claim Citation Source Quote Verdict Transformers use self-attention [arxiv:1706.03762] \"The Transformer... relies entirely on self-attention\" \u2705 SUPPORTED Allow parallel processing [arxiv:1706.03762] \"...allows for significantly more parallelization\" \u2705 SUPPORTED Multi-head attention splits input [arxiv:1706.03762] \"Multi-head attention allows the model to jointly attend to information from different representation subspaces\" \u2705 SUPPORTED <p>Verification Summary: - 3 claims checked - 3 supported, 0 not found - All citations valid</p>"},{"location":"anti-hallucination/#stage-3-self-critique-optional","title":"Stage 3: Self-Critique (Optional)","text":"<p>A senior reviewer evaluates completeness, accuracy, and clarity with improvement suggestions.</p>"},{"location":"anti-hallucination/#goal_2","title":"Goal","text":"<p>High-level quality assessment: - Is the answer complete? - Are all aspects of the question addressed? - Is the explanation clear? - What could be improved?</p>"},{"location":"anti-hallucination/#output-format_2","title":"Output Format","text":"<p>The critique stage produces:</p> <pre><code>## Critique Report\n\n**Completeness:** \u2b50\u2b50\u2b50\u2b50 (4/5)\nThe answer covers the core concepts well but could mention the positional\nencoding mechanism.\n\n**Accuracy:** \u2b50\u2b50\u2b50\u2b50\u2b50 (5/5)\nAll claims are correctly sourced and verified. Citations are appropriate.\n\n**Clarity:** \u2b50\u2b50\u2b50\u2b50 (4/5)\nExplanation is clear but could benefit from a concrete example.\n\n**Suggestions for Improvement:**\n1. Add discussion of positional encoding\n2. Include example of attention weights calculation\n3. Compare to previous sequence models (RNN, LSTM)\n</code></pre>"},{"location":"anti-hallucination/#paper-deep-dive-mode","title":"Paper Deep-Dive Mode","text":"<p>When analyzing a specific paper (<code>paper</code> or <code>papers</code> command), the system adds extra grounding:</p> <pre><code>IMPORTANT: You are analysing a SPECIFIC research paper.\nONLY use information from the paper excerpts below.\nDo NOT add information from your training data.\nIf the paper does not mention something, say so.\nCite specific sections, equations, figures, or tables.\n</code></pre>"},{"location":"anti-hallucination/#why-this-matters","title":"Why This Matters","text":"<p>When users ask about a specific paper, they want: - Information FROM that paper only - Not general knowledge about the topic - Explicit \"not mentioned\" if the paper lacks info - References to specific sections/equations/figures</p> <p>Example Difference:</p> <p>General mode:</p> <p>\"Transformers use self-attention mechanisms. They were introduced in 2017 and have become the dominant architecture in NLP.\"</p> <p>Paper deep-dive mode:</p> <p>\"According to this paper [Section 3.2], the Transformer uses multi-head self-attention. The paper does NOT discuss historical context or comparison to previous work. See Figure 2 for the architecture diagram.\"</p>"},{"location":"anti-hallucination/#citation-summary-display","title":"Citation Summary Display","text":"<p>After all stages complete, the system shows a compact summary:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 CITATION VERIFICATION                               \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n\u2705 3 claims verified\n\u2705 All citations supported\n\ud83d\udcc4 Sources: arxiv:1706.03762\n</code></pre> <p>This provides quick confidence without overwhelming the user with the full verification table.</p>"},{"location":"anti-hallucination/#next-api-reference","title":"Next: API Reference","text":"<p>See API Reference for detailed configuration and CLI options.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete reference for environment variables, CLI commands, and dependencies.</p>"},{"location":"api-reference/#environment-variables","title":"Environment Variables","text":"Variable Required Default Description <code>SCALEDOWN_API_KEY</code> \u2705 Yes \u2014 Your ScaleDown API key from ScaleDown <code>GEMINI_API_KEY</code> \u2705 Yes \u2014 Your Google Gemini API key from AI Studio <code>SCALEDOWN_MODEL</code> No <code>gemini-2.5-flash</code> Target model for compression optimization <code>GEMINI_MODEL</code> No <code>gemini-2.5-flash</code> Gemini model for generation <code>SCALEDOWN_TIMEOUT</code> No <code>15</code> Timeout (seconds) for ScaleDown API calls <code>CHUNK_SIZE</code> No <code>1000</code> Characters per text chunk <code>CHUNK_OVERLAP</code> No <code>200</code> Overlap between adjacent chunks <code>TOP_K</code> No <code>5</code> Number of chunks to retrieve per query"},{"location":"api-reference/#cli-commands","title":"CLI Commands","text":""},{"location":"api-reference/#ask-research-questions","title":"<code>ask</code> \u2014 Research Questions","text":"<pre><code>python -m src.main ask \"question\" [--session SESSION_ID]\n</code></pre> <p>Arguments: - <code>question</code> (required): The research question to answer - <code>--session SESSION_ID</code> (optional): Continue an existing session</p> <p>What it does: 1. Triage question complexity 2. Discover and download relevant papers (if research question) 3. Chunk, index, and retrieve relevant content 4. Compress context via ScaleDown 5. Run full reasoning pipeline (COT \u2192 Verify \u2192 Critique) 6. Save session for follow-ups</p> <p>Example: <pre><code>python -m src.main ask \"What are the latest advances in neural architecture search?\"\npython -m src.main ask \"How does this compare to random search?\" --session abc123\n</code></pre></p>"},{"location":"api-reference/#papers-interactive-paper-explorer","title":"<code>papers</code> \u2014 Interactive Paper Explorer","text":"<pre><code>python -m src.main papers \"search_query\"\n</code></pre> <p>Arguments: - <code>search_query</code> (required): ArXiv search query</p> <p>Interactive commands: - Type text: Ask a question about the selected paper - Type number: Switch to a different paper - <code>back</code>: Return to paper list - <code>list</code>: Show paper list again - <code>s</code>: New search - <code>q</code>: Quit</p> <p>What it does: - Search ArXiv and display results - Let you select a paper - Answer questions using full pipeline - Maintain session across all interactions - Cache papers (no refetching)</p> <p>Example: <pre><code>python -m src.main papers \"attention mechanism transformers\"\n</code></pre></p>"},{"location":"api-reference/#paper-specific-paper-analysis","title":"<code>paper</code> \u2014 Specific Paper Analysis","text":"<pre><code>python -m src.main paper &lt;arxiv_id&gt; \"question\"\n</code></pre> <p>Arguments: - <code>arxiv_id</code> (required): ArXiv ID (e.g., <code>1706.03762</code>) - <code>question</code> (required): Question about the paper</p> <p>What it does: 1. Download paper (if not cached) 2. Extract and index text 3. Run full pipeline with paper-specific grounding 4. Answer using ONLY information from this paper</p> <p>Example: <pre><code>python -m src.main paper 1706.03762 \"What is the multi-head attention mechanism?\"\n</code></pre></p>"},{"location":"api-reference/#search-quick-arxiv-search","title":"<code>search</code> \u2014 Quick ArXiv Search","text":"<pre><code>python -m src.main search \"query\"\n</code></pre> <p>Arguments: - <code>query</code> (required): ArXiv search query</p> <p>What it does: - Search ArXiv and display top 10 results - Show titles, authors, abstracts - No download or analysis</p> <p>Example: <pre><code>python -m src.main search \"graph neural networks\"\n</code></pre></p>"},{"location":"api-reference/#sessions-list-conversations","title":"<code>sessions</code> \u2014 List Conversations","text":"<pre><code>python -m src.main sessions\n</code></pre> <p>What it does: - List all saved session IDs - Show creation timestamps - Display number of Q&amp;A turns - List papers in each session</p> <p>Example output: <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Session ID \u2503 Created            \u2503 Turns \u2503 Papers             \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 abc123     \u2502 2024-01-15 10:30   \u2502 3     \u2502 arxiv:1706.03762   \u2502\n\u2502 def456     \u2502 2024-01-15 14:22   \u2502 1     \u2502 arxiv:2103.14030   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api-reference/#workflow-show-display-pipeline-config","title":"<code>workflow show</code> \u2014 Display Pipeline Config","text":"<pre><code>python -m src.main workflow show\n</code></pre> <p>What it does: - Show all pipeline stages - Display enabled/disabled status - Show execution order</p> <p>Example output: <pre><code>Current Workflow:\n1. cot (enabled) \u2014 Chain-of-thought reasoning\n2. self_verify (enabled) \u2014 Citation verification\n3. self_critique (enabled) \u2014 Quality evaluation\n</code></pre></p>"},{"location":"api-reference/#workflow-toggle-enabledisable-stages","title":"<code>workflow toggle</code> \u2014 Enable/Disable Stages","text":"<pre><code>python -m src.main workflow toggle &lt;stage&gt; &lt;on|off&gt;\n</code></pre> <p>Arguments: - <code>stage</code> (required): <code>cot</code>, <code>self_verify</code>, or <code>self_critique</code> - <code>on|off</code> (required): Enable or disable</p> <p>Example: <pre><code>python -m src.main workflow toggle self_critique off\npython -m src.main workflow toggle self_verify on\n</code></pre></p>"},{"location":"api-reference/#workflow-reorder-change-stage-order","title":"<code>workflow reorder</code> \u2014 Change Stage Order","text":"<pre><code>python -m src.main workflow reorder &lt;stage1,stage2,...&gt;\n</code></pre> <p>Arguments: - Comma-separated list of stages in desired order</p> <p>Example: <pre><code>python -m src.main workflow reorder cot,self_verify,self_critique\npython -m src.main workflow reorder cot,self_critique,self_verify\n</code></pre></p>"},{"location":"api-reference/#artifacts-list-view-stored-outputs","title":"<code>artifacts list</code> \u2014 View Stored Outputs","text":"<pre><code>python -m src.main artifacts list\n</code></pre> <p>What it does: - List all stored reasoning artifacts - Show artifact IDs, stages, timestamps, sizes</p> <p>Example output: <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Artifact ID  \u2503 Stage        \u2503 Timestamp          \u2503 Size  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 abc123       \u2502 cot          \u2502 2024-01-15 10:30   \u2502 2.3KB \u2502\n\u2502 abc123       \u2502 self_verify  \u2502 2024-01-15 10:31   \u2502 1.1KB \u2502\n\u2502 abc123       \u2502 self_critique\u2502 2024-01-15 10:32   \u2502 0.8KB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api-reference/#dependencies","title":"Dependencies","text":"Package Version Purpose <code>requests</code> Latest HTTP client for ScaleDown, Gemini, ArXiv APIs <code>python-dotenv</code> Latest Load <code>.env</code> environment variables <code>numpy</code> Latest Array operations for TF-IDF calculations <code>scikit-learn</code> Latest TF-IDF vectorizer, cosine similarity <code>PyPDF2</code> Latest PDF text extraction from ArXiv papers <code>rich</code> Latest Terminal UI (tables, panels, markdown, spinners) <p>Install all: <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"api-reference/#next-project-structure","title":"Next: Project Structure","text":"<p>See Project Structure for codebase organization.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The Scientific Literature Explorer uses a modular pipeline architecture that combines retrieval-augmented generation, context compression, and multi-stage verification to produce accurate, well-cited answers from scientific literature.</p>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<pre><code>%%{init: {'theme':'base','themeVariables':{'fontFamily':'sans-serif'}}}%%\nflowchart TD\n    Start([User Question]) --&gt; Triage[Research Agent&lt;br/&gt;Triage + Keywords&lt;br/&gt;Gemini]\n\n    Triage --&gt;|General Question| DirectAnswer[Direct Answer&lt;br/&gt;Gemini COT]\n    Triage --&gt;|Research Question| ArXiv[ArXiv API Search]\n\n    ArXiv --&gt; PDFs[Parallel PDF Download&lt;br/&gt;&amp; Text Extraction&lt;br/&gt;PyPDF2]\n    PDFs --&gt; RAG[RAG Pipeline&lt;br/&gt;Chunk \u2192 TF-IDF \u2192 Retrieve]\n\n    RAG --&gt; Compress[ScaleDown API&lt;br/&gt;Context Compression&lt;br/&gt;40-60% reduction]\n\n    Compress --&gt; Workflow[Anti-Hallucination Workflow]\n    DirectAnswer --&gt; Workflow\n\n    subgraph Workflow [Reasoning Pipeline]\n        COT[Chain-of-Thought&lt;br/&gt;Gemini 2048 tokens&lt;br/&gt;Strict Citations]\n        Verify[Self-Verification&lt;br/&gt;Gemini Fast 1024 tokens&lt;br/&gt;Citation Check]\n        Critique[Self-Critique&lt;br/&gt;Gemini Fast 1024 tokens&lt;br/&gt;Quality Review]\n\n        COT --&gt; Verify\n        Verify --&gt; Critique\n    end\n\n    Workflow --&gt; Store[Artifact Storage&lt;br/&gt;Compressed Markdown]\n    Store --&gt; Session[(Session JSON&lt;br/&gt;Conversation History)]\n\n    Session --&gt; Output([Answer + Citations])\n\n    style Start fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Output fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Compress fill:#f57c00,stroke:#e65100,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Workflow fill:#7b1fa2,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Triage fill:#d32f2f,stroke:#b71c1c,stroke-width:3px,color:#fff,rx:5,ry:5\n    style DirectAnswer fill:#0288d1,stroke:#01579b,stroke-width:2px,color:#fff,rx:5,ry:5\n    style ArXiv fill:#0288d1,stroke:#01579b,stroke-width:2px,color:#fff,rx:5,ry:5\n    style PDFs fill:#0288d1,stroke:#01579b,stroke-width:2px,color:#fff,rx:5,ry:5\n    style RAG fill:#0288d1,stroke:#01579b,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Store fill:#388e3c,stroke:#1b5e20,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Session fill:#5d4037,stroke:#3e2723,stroke-width:2px,color:#fff,rx:5,ry:5\n\n    linkStyle default stroke:#e0e0e0,stroke-width:2px</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-research-agent-triage-discovery","title":"1. Research Agent (Triage + Discovery)","text":"<p>Purpose: Intelligently routes questions and discovers relevant papers</p> <ul> <li>Question Classification: Uses Gemini to classify as <code>general</code>, <code>conceptual</code>, or <code>research</code></li> <li>Keyword Extraction: Extracts search terms and ArXiv query in the same API call</li> <li>Paper Discovery: Searches ArXiv Atom API with parallel PDF downloads</li> <li>Fallback Handling: Uses heuristic keyword extraction if Gemini is rate-limited</li> </ul>"},{"location":"architecture/#2-rag-pipeline","title":"2. RAG Pipeline","text":"<p>Purpose: Retrieves and prepares relevant paper content</p> <ul> <li>Chunking: Splits papers into overlapping segments (1000 chars, 200 overlap)</li> <li>TF-IDF Indexing: scikit-learn vectorization with stop-word removal</li> <li>Retrieval: Cosine similarity matching, returns top-k chunks (default: 5)</li> <li>Source Tracking: Every chunk labeled with <code>arxiv:XXXX.XXXXX</code> for citations</li> </ul>"},{"location":"architecture/#3-scaledown-compression","title":"3. ScaleDown Compression","text":"<p>Purpose: Reduces token count while preserving semantic meaning</p> <ul> <li>Context Compression: 40-60% token reduction before sending to LLM</li> <li>Query-Aware: Uses the user's question to guide what information to preserve</li> <li>Tokenizer Optimization: Optimized for <code>gemini-2.5-flash</code> tokenizer</li> <li>Artifact Compression: Also compresses COT traces before storage</li> </ul>"},{"location":"architecture/#4-anti-hallucination-workflow","title":"4. Anti-Hallucination Workflow","text":"<p>Purpose: Multi-stage verification ensures factual accuracy</p> <ul> <li>Chain-of-Thought (COT): Requires inline citations for every claim</li> <li>Self-Verification: Separate Gemini call checks each citation</li> <li>Self-Critique: Optional quality evaluation</li> <li>Configurable: Stages can be toggled or reordered via CLI</li> </ul>"},{"location":"architecture/#5-artifact-storage","title":"5. Artifact Storage","text":"<p>Purpose: Persistent storage of reasoning traces</p> <ul> <li>Markdown Format: Each stage output saved as <code>.md</code> file</li> <li>Compression: Artifacts compressed via ScaleDown before storage</li> <li>Metadata: Timestamps, token counts, compression stats tracked</li> <li>Organization: Separate folders for <code>cot/</code>, <code>self_verify/</code>, <code>self_critique/</code></li> </ul>"},{"location":"architecture/#6-session-management","title":"6. Session Management","text":"<p>Purpose: Maintains conversation history across interactions</p> <ul> <li>JSON Persistence: Each session stored as <code>{session_id}.json</code></li> <li>Multi-Turn Support: Previous Q&amp;A included in context</li> <li>Paper Tracking: Tracks which papers were ingested per session</li> <li>Automatic Loading: Latest session auto-loaded if applicable</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#question-processing-flow","title":"Question Processing Flow","text":"<ol> <li>User submits question \u2192 Research Agent</li> <li>Triage classification \u2192 Routes to appropriate handler</li> <li>Paper discovery (if needed) \u2192 ArXiv API + parallel PDF downloads</li> <li>Text extraction \u2192 PyPDF2 processes PDFs</li> <li>Chunking + Indexing \u2192 TF-IDF vectorization</li> <li>Retrieval \u2192 Top-k chunks by cosine similarity</li> <li>Compression \u2192 ScaleDown reduces token count</li> <li>Reasoning \u2192 Multi-stage workflow generates answer</li> <li>Storage \u2192 Artifacts saved, session updated</li> <li>Response \u2192 Final answer with citations returned</li> </ol>"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":"Layer Technology Purpose Intelligence Google Gemini 2.5 Flash Answer generation, classification, verification Compression ScaleDown API Context compression, fallback generation Paper Source ArXiv Atom API Scientific paper search and metadata PDF Processing PyPDF2 Text extraction from PDFs Retrieval scikit-learn TF-IDF Vectorization and similarity search Storage JSON (sessions), Markdown (artifacts) Persistence CLI Rich (terminal UI) Interactive tables, panels, markdown rendering"},{"location":"architecture/#next-how-it-works","title":"Next: How It Works","text":"<p>See How It Works for the detailed end-to-end flow and step-by-step breakdown.</p>"},{"location":"configuration/","title":"Configuration Reference","text":"<p>All configuration is managed through environment variables loaded from the <code>.env</code> file.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/#required-variables","title":"Required Variables","text":"Variable Description <code>SCALEDOWN_API_KEY</code> Required. Your ScaleDown API key from ScaleDown. Used for context compression and fallback generation. <code>GEMINI_API_KEY</code> Required. Your Google Gemini API key from AI Studio. Used for question triage, keyword extraction, COT reasoning, verification, and critique."},{"location":"configuration/#optional-configuration","title":"Optional Configuration","text":"Variable Default Description <code>SCALEDOWN_MODEL</code> <code>gemini-2.5-flash</code> Target model for ScaleDown compression optimization. ScaleDown will optimize the tokenization for this specific model's tokenizer. Valid values: <code>gpt-4o</code>, <code>claude-3-5-sonnet</code>, <code>gemini-2.5-flash</code>, etc. <code>GEMINI_MODEL</code> <code>gemini-2.5-flash</code> The Gemini model to use for generation. Options: <code>gemini-2.5-flash</code>, <code>gemini-1.5-flash</code>, <code>gemini-1.5-pro</code>, etc. Flash models are faster/cheaper but less capable than Pro models. <code>SCALEDOWN_TIMEOUT</code> <code>15</code> Timeout in seconds for ScaleDown API calls. Increase if you see timeout errors on slow networks. <code>CHUNK_SIZE</code> <code>1000</code> Number of characters per text chunk when splitting papers. Larger chunks = more context per chunk but fewer chunks retrieved. <code>CHUNK_OVERLAP</code> <code>200</code> Number of overlapping characters between adjacent chunks. Prevents information loss at chunk boundaries. <code>TOP_K</code> <code>5</code> Number of chunks to retrieve per RAG query. Higher = more context but also more tokens and potential noise."},{"location":"configuration/#model-selection","title":"Model Selection","text":""},{"location":"configuration/#scaledown-model","title":"ScaleDown Model","text":"<p>The <code>SCALEDOWN_MODEL</code> variable tells ScaleDown which tokenizer to optimize for. Use: - <code>gemini-2.5-flash</code> if you're using Gemini 2.5 Flash (default) - <code>gpt-4o</code> if you're using OpenAI's GPT-4o - <code>claude-3-5-sonnet</code> if you're using Claude 3.5 Sonnet</p> <p>This does NOT change which model ScaleDown uses internally \u2014 it only optimizes the compression output for your target model's tokenizer.</p>"},{"location":"configuration/#gemini-model","title":"Gemini Model","text":"<p>The <code>GEMINI_MODEL</code> variable selects which Gemini model to use for generation: - <code>gemini-2.5-flash</code> (default): Fastest, cheapest, good quality - <code>gemini-1.5-flash</code>: Previous generation, slower than 2.5 - <code>gemini-1.5-pro</code>: Much smarter but slower and more expensive</p>"},{"location":"configuration/#rag-configuration","title":"RAG Configuration","text":""},{"location":"configuration/#chunk-size","title":"Chunk Size","text":"<p>The <code>CHUNK_SIZE</code> controls how large each text chunk is: - Too small (e.g., 200): Chunks lose semantic meaning, context fragmentation - Too large (e.g., 5000): Fewer chunks retrieved, may miss relevant details - Default 1000: Good balance for most scientific papers</p>"},{"location":"configuration/#chunk-overlap","title":"Chunk Overlap","text":"<p>The <code>CHUNK_OVERLAP</code> ensures no information is lost at boundaries: - No overlap (0): Risk of splitting sentences/paragraphs - Too much overlap (500+): Redundant content, wasted tokens - Default 200: Usually captures 1-2 sentences of overlap</p>"},{"location":"configuration/#top-k","title":"Top-K","text":"<p>The <code>TOP_K</code> controls how many chunks are retrieved: - Too few (&lt;3): May miss important information - Too many (&gt;10): More noise, higher token costs - Default 5: Works well for most questions</p>"},{"location":"configuration/#example-env","title":"Example <code>.env</code>","text":"<pre><code># Required\nSCALEDOWN_API_KEY=sk_sd_abc123...\nGEMINI_API_KEY=AIza...\n\n# Optional - Uncomment to override defaults\n# SCALEDOWN_MODEL=gemini-2.5-flash\n# GEMINI_MODEL=gemini-2.5-flash\n# SCALEDOWN_TIMEOUT=15\n# CHUNK_SIZE=1000\n# CHUNK_OVERLAP=200\n# TOP_K=5\n</code></pre>"},{"location":"configuration/#next-usage-guide","title":"Next: Usage Guide","text":"<p>See Usage Guide for all available commands and examples.</p>"},{"location":"examples/","title":"Usage Examples","text":"<p>Practical examples demonstrating common workflows and use cases.</p>"},{"location":"examples/#example-1-quick-research","title":"Example 1: Quick Research","text":""},{"location":"examples/#scenario","title":"Scenario","text":"<p>You need to quickly understand a topic from recent research papers.</p>"},{"location":"examples/#command","title":"Command","text":"<pre><code>python -m src.main ask \"What is neural architecture search?\"\n</code></pre>"},{"location":"examples/#what-happens","title":"What Happens","text":"<ol> <li>\u26a1 Question triaged as \"research\"</li> <li>\ud83d\udd0d ArXiv searched for relevant papers (top 5 results)</li> <li>\ud83d\udce5 PDFs downloaded in parallel (~10-15s)</li> <li>\u2702\ufe0f Text extracted and chunked (1000 chars, 200 overlap)</li> <li>\ud83d\udcca Top-5 chunks retrieved via TF-IDF</li> <li>\ud83d\udddc\ufe0f Context compressed via ScaleDown (1500 \u2192 600 tokens)</li> <li>\ud83e\udde0 COT reasoning with inline citations</li> <li>\u2705 Self-verification checks all citations</li> <li>\ud83d\udccb Self-critique evaluates quality</li> <li>\ud83d\udcbe Session saved with ID <code>abc123</code></li> </ol>"},{"location":"examples/#expected-result","title":"Expected Result","text":"<pre><code># Scientific Literature Explorer\n\n## Answer\n\nNeural architecture search (NAS) is an automated method for discovering\noptimal neural network architectures [arxiv:1808.05377]. Unlike manual\ndesign, NAS uses algorithms such as reinforcement learning or evolutionary\nstrategies to explore the architecture search space [arxiv:1808.05377].\n\nDARTS (Differentiable Architecture Search) improved efficiency by making\nthe search space continuous and differentiable [arxiv:1806.09055]. This\nallows gradient-based optimization instead of discrete search methods.\n\n## References\n\n- [arxiv:1808.05377] Elsken et al., \"Neural Architecture Search: A Survey\"\n- [arxiv:1806.09055] Liu et al., \"DARTS: Differentiable Architecture Search\"\n\n## Verification Summary\n\u2705 2 claims verified\n\u2705 All citations supported\n\ud83d\udcc4 Sources: arxiv:1808.05377, arxiv:1806.09055\n</code></pre> <p>Time: ~45-60 seconds</p>"},{"location":"examples/#example-2-multi-turn-conversation","title":"Example 2: Multi-Turn Conversation","text":""},{"location":"examples/#scenario_1","title":"Scenario","text":"<p>You want to ask follow-up questions using the same papers.</p>"},{"location":"examples/#commands","title":"Commands","text":"<pre><code># First question\npython -m src.main ask \"What is neural architecture search?\"\n# Note the session ID: abc123\n\n# Follow-up #1\npython -m src.main ask \"What are the main challenges?\" --session abc123\n\n# Follow-up #2\npython -m src.main ask \"How does DARTS address these?\" --session abc123\n</code></pre>"},{"location":"examples/#what-happens_1","title":"What Happens","text":"<ul> <li>First question: Full pipeline (~45-60s)</li> <li>Follow-ups: Papers cached, faster pipeline (~20-30s)</li> <li>Each answer has context from previous Q&amp;A</li> </ul>"},{"location":"examples/#expected-flow","title":"Expected Flow","text":"<pre><code>Q1: \"What is neural architecture search?\"\n\u2192 Papers downloaded: arxiv:1808.05377, arxiv:1806.09055, arxiv:1802.03268\n\u2192 Session abc123 created\n\nQ2: \"What are the main challenges?\" --session abc123\n\u2192 Papers already cached (no download)\n\u2192 LLM sees Q1 + A1 as context\n\u2192 Answer focuses on challenges (computational cost, search space size)\n\nQ3: \"How does DARTS address these?\" --session abc123\n\u2192 Papers still cached\n\u2192 LLM sees Q1+A1, Q2+A2 as context\n\u2192 Answer connects DARTS from Q1 to challenges from Q2\n</code></pre> <p>Total Time: Q1 (60s) + Q2 (25s) + Q3 (25s) = ~110s</p>"},{"location":"examples/#example-3-interactive-paper-explorer","title":"Example 3: Interactive Paper Explorer","text":""},{"location":"examples/#scenario_2","title":"Scenario","text":"<p>You want to browse multiple papers and ask questions about specific ones.</p>"},{"location":"examples/#command_1","title":"Command","text":"<pre><code>python -m src.main papers \"transformers attention mechanism\"\n</code></pre>"},{"location":"examples/#interactive-session","title":"Interactive Session","text":"<pre><code>&gt; papers \"transformers attention mechanism\"\n\nPapers found:\n1. [arxiv:1706.03762] Attention Is All You Need (Vaswani et al., 2017)\n2. [arxiv:2002.04745] Reformer: The Efficient Transformer (Kitaev et al., 2020)\n3. [arxiv:2006.04768] Longformer (Beltagy et al., 2020)\n...\n\nSelect paper (1-10), or 's' to search, 'q' to quit: 1\n\n[Paper #1 selected: Attention Is All You Need]\n\nAsk question (or 'back', 'q'): What is the multi-head attention mechanism?\n\n[Full pipeline runs with paper-specific grounding]\n\n## Answer\nThe multi-head attention mechanism splits the input into h=8 parallel\nattention heads [arxiv:1706.03762, Section 3.2.2]. Each head learns\ndifferent representation subspaces...\n\nAsk question (or 'back', 'q'): How many parameters does the base model have?\n\n[Follow-up answered instantly using cached paper]\n\n## Answer\nThe base Transformer model has 65M parameters [arxiv:1706.03762, Table 3]...\n\nAsk question (or 'back', 'q'): back\n\n[Returns to paper list]\n\nSelect paper (1-10), or 's' to search, 'q' to quit: 2\n\n[Switches to Paper #2: Reformer]\n\nAsk question (or 'back', 'q'): How does this compare to the original Transformer?\n\n[Pipeline runs comparing Reformer to Transformer]\n\nAsk question (or 'back', 'q'): q\n</code></pre> <p>Features: - Seamless paper switching - All interactions share one session - Context-aware answers (remembers previous questions) - No refetching on follow-ups</p>"},{"location":"examples/#example-4-specific-paper-analysis","title":"Example 4: Specific Paper Analysis","text":""},{"location":"examples/#scenario_3","title":"Scenario","text":"<p>You know the exact ArXiv ID and want to analyze that paper only.</p>"},{"location":"examples/#command_2","title":"Command","text":"<pre><code>python -m src.main paper 1706.03762 \"Explain the positional encoding mechanism\"\n</code></pre>"},{"location":"examples/#what-happens_2","title":"What Happens","text":"<ol> <li>Downloads paper <code>1706.03762</code> (if not cached)</li> <li>Extracts text</li> <li>Runs pipeline with paper-specific grounding</li> <li>Answer uses ONLY information from this paper</li> </ol>"},{"location":"examples/#paper-specific-grounding","title":"Paper-Specific Grounding","text":"<p>System prompt includes: <pre><code>IMPORTANT: You are analyzing a SPECIFIC research paper.\nONLY use information from the paper excerpts below.\nDo NOT add information from your training data.\nIf the paper does not mention something, say so.\nCite specific sections, equations, figures, or tables.\n</code></pre></p>"},{"location":"examples/#expected-result_1","title":"Expected Result","text":"<pre><code>## Answer\n\nAccording to Section 3.5, the positional encoding uses sinusoidal functions:\n\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\nwhere pos is the position and i is the dimension. This allows the model\nto learn relative positions [arxiv:1706.03762, Section 3.5].\n\nThe paper states that this method was chosen because it allows the model\nto extrapolate to sequence lengths longer than those seen during training\n[arxiv:1706.03762, Section 3.5].\n\nThe paper does NOT provide ablation studies comparing sinusoidal encoding\nto learned positional embeddings.\n\n## References\n- [arxiv:1706.03762] Section 3.5 (Positional Encoding)\n</code></pre> <p>Time: ~30-40 seconds</p>"},{"location":"examples/#example-5-configure-workflow","title":"Example 5: Configure Workflow","text":""},{"location":"examples/#scenario_4","title":"Scenario","text":"<p>You want faster responses and don't need critique.</p>"},{"location":"examples/#commands_1","title":"Commands","text":"<pre><code># Check current workflow\npython -m src.main workflow show\n\n# Disable critique\npython -m src.main workflow toggle self_critique off\n\n# Ask question (skips critique stage)\npython -m src.main ask \"What is gradient descent?\"\n\n# Re-enable critique later\npython -m src.main workflow toggle self_critique on\n</code></pre>"},{"location":"examples/#expected-output","title":"Expected Output","text":"<pre><code>$ python -m src.main workflow show\n\nCurrent Workflow:\n1. cot (enabled) \u2014 Chain-of-thought reasoning\n2. self_verify (enabled) \u2014 Citation verification\n3. self_critique (enabled) \u2014 Quality evaluation\n\n$ python -m src.main workflow toggle self_critique off\n\n\u2705 Disabled stage: self_critique\n\n$ python -m src.main workflow show\n\nCurrent Workflow:\n1. cot (enabled) \u2014 Chain-of-thought reasoning\n2. self_verify (enabled) \u2014 Citation verification\n3. self_critique (disabled) \u2014 Quality evaluation\n</code></pre> <p>Speed Improvement: ~5-8 seconds saved per question</p>"},{"location":"examples/#example-6-artifact-management","title":"Example 6: Artifact Management","text":""},{"location":"examples/#scenario_5","title":"Scenario","text":"<p>You want to review the stored reasoning artifacts.</p>"},{"location":"examples/#command_3","title":"Command","text":"<pre><code>python -m src.main artifacts list\n</code></pre>"},{"location":"examples/#expected-output_1","title":"Expected Output","text":"<pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Artifact ID  \u2503 Stage        \u2503 Timestamp          \u2503 Size  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 abc123       \u2502 cot          \u2502 2024-01-15 10:30   \u2502 2.3KB \u2502\n\u2502 abc123       \u2502 self_verify  \u2502 2024-01-15 10:31   \u2502 1.1KB \u2502\n\u2502 abc123       \u2502 self_critique\u2502 2024-01-15 10:32   \u2502 0.8KB \u2502\n\u2502 def456       \u2502 cot          \u2502 2024-01-15 14:45   \u2502 3.1KB \u2502\n\u2502 def456       \u2502 self_verify  \u2502 2024-01-15 14:46   \u2502 1.4KB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Files are stored in <code>artifacts/</code>: - <code>artifacts/cot/abc123.md</code> \u2014 Chain-of-thought reasoning - <code>artifacts/self_verify/abc123.md</code> \u2014 Verification table - <code>artifacts/self_critique/abc123.md</code> \u2014 Critique report</p> <p>You can open these files in any text editor to review the full reasoning trace.</p>"},{"location":"examples/#example-7-session-management","title":"Example 7: Session Management","text":""},{"location":"examples/#scenario_6","title":"Scenario","text":"<p>You want to see all your previous conversations.</p>"},{"location":"examples/#command_4","title":"Command","text":"<pre><code>python -m src.main sessions\n</code></pre>"},{"location":"examples/#expected-output_2","title":"Expected Output","text":"<pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Session ID \u2503 Created            \u2503 Turns \u2503 Papers             \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 abc123     \u2502 2024-01-15 10:30   \u2502 3     \u2502 arxiv:1706.03762   \u2502\n\u2502            \u2502                    \u2502       \u2502 arxiv:1808.05377   \u2502\n\u2502 def456     \u2502 2024-01-15 14:22   \u2502 1     \u2502 arxiv:2103.14030   \u2502\n\u2502 ghi789     \u2502 2024-01-16 09:15   \u2502 5     \u2502 arxiv:1806.09055   \u2502\n\u2502            \u2502                    \u2502       \u2502 arxiv:1802.03268   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Use any session ID to continue a conversation: <pre><code>python -m src.main ask \"Tell me more about the attention mechanism\" --session abc123\n</code></pre></p>"},{"location":"examples/#next-methodology","title":"Next: Methodology","text":"<p>See Methodology to understand the technical approach behind these workflows.</p>"},{"location":"how-it-works/","title":"How It Works","text":"<p>This page explains the end-to-end flow of the Scientific Literature Explorer from user question to final answer.</p>"},{"location":"how-it-works/#end-to-end-flow","title":"End-to-End Flow","text":"<p>The system follows a 9-step pipeline that intelligently routes questions, discovers papers, retrieves relevant content, compresses context, and runs multi-stage verification.</p>"},{"location":"how-it-works/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":"<ol> <li> <p>Question Triage \u2014 The Research Agent classifies the question as <code>general</code>, <code>conceptual</code>, or <code>research</code> using Gemini. General questions (e.g., \"What is CNN?\") are answered directly without paper retrieval, saving significant latency.</p> </li> <li> <p>Keyword Extraction \u2014 In the same Gemini call as triage, search keywords and an ArXiv query are extracted. This merged call halves the initial latency.</p> </li> <li> <p>Paper Discovery \u2014 The system searches ArXiv's Atom API for relevant papers. PDFs are downloaded in parallel using <code>ThreadPoolExecutor</code> and text is extracted via PyPDF2.</p> </li> <li> <p>Chunking &amp; Indexing \u2014 Paper text is split into overlapping chunks (default: 1000 chars, 200 overlap) and indexed using TF-IDF vectorization (scikit-learn).</p> </li> <li> <p>Retrieval \u2014 The top-k most relevant chunks are retrieved using cosine similarity against the TF-IDF matrix.</p> </li> <li> <p>Context Compression \u2014 Retrieved chunks are compressed through the ScaleDown API, reducing token count while preserving semantic meaning. This is critical for fitting more information into the LLM's context window at lower cost.</p> </li> <li> <p>Reasoning Workflow \u2014 The compressed context passes through a configurable multi-stage pipeline:</p> </li> <li>Chain-of-Thought (COT): Gemini generates a detailed, step-by-step analysis with mandatory inline citations</li> <li>Self-Verification: A faster Gemini call checks every citation against the source documents</li> <li> <p>Self-Critique (optional): Evaluates completeness, accuracy, and clarity</p> </li> <li> <p>Artifact Storage \u2014 Each stage's output is saved as compressed markdown with metadata (timestamps, token counts, compression stats).</p> </li> <li> <p>Session Persistence \u2014 Conversations are saved as JSON, enabling multi-turn interactions with history context.</p> </li> </ol>"},{"location":"how-it-works/#role-of-scaledown-api","title":"Role of ScaleDown API","text":"<p>ScaleDown is the context compression backbone of this system. It is NOT an LLM \u2014 it's a specialized service that intelligently compresses text while preserving semantic meaning.</p>"},{"location":"how-it-works/#what-scaledown-does","title":"What ScaleDown Does","text":"Function How It's Used Context Compression Retrieved RAG chunks are compressed before being sent to Gemini, reducing token usage by 40-60% Query-Aware Compression The user's question is passed as the <code>prompt</code> parameter, so ScaleDown preserves information most relevant to the query Artifact Compression COT traces, verification tables, and critique outputs are compressed before storage Fallback Generation When Gemini is rate-limited (429), ScaleDown's compression endpoint (which internally uses GPT-4o) is used as a pseudo-generation fallback"},{"location":"how-it-works/#scaledown-api-endpoint","title":"ScaleDown API Endpoint","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#f57c00','primaryTextColor':'#fff','primaryBorderColor':'#e65100','lineColor':'#424242'}}}%%\nsequenceDiagram\n    participant Client\n    participant SD as ScaleDown API\n\n    Client-&gt;&gt;SD: POST /compress/raw/\n    Note over Client,SD: Headers: x-api-key&lt;br/&gt;Content-Type: application/json\n\n    Note left of Client: Payload&lt;br/&gt;context: 1500 tokens&lt;br/&gt;prompt: user question&lt;br/&gt;model: gemini-2.5-flash&lt;br/&gt;scaledown rate: auto\n\n    SD-&gt;&gt;SD: Analyze context + prompt relevance\n    SD-&gt;&gt;SD: Apply query-aware compression\n    SD-&gt;&gt;SD: Optimize for target model tokenizer\n\n    SD--&gt;&gt;Client: Response (2.3s)\n    Note right of SD: compressed_prompt: 600 tokens&lt;br/&gt;original_tokens: 1500&lt;br/&gt;compressed_tokens: 600&lt;br/&gt;successful: true&lt;br/&gt;latency_ms: 2341\n\n    Note over Client,SD: 60% reduction in token count</code></pre> <p>Raw API Example:</p> <pre><code>curl -X POST https://api.scaledown.xyz/compress/raw/ \\\n  -H \"x-api-key: YOUR_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"context\": \"The text to compress...\",\n    \"prompt\": \"The user question (guides compression)\",\n    \"model\": \"gemini-2.5-flash\",\n    \"scaledown\": { \"rate\": \"auto\" }\n  }'\n</code></pre>"},{"location":"how-it-works/#scaledown-as-fallback","title":"ScaleDown as Fallback","text":"<p>When Gemini hits its rate limit (429 errors), the system automatically falls back to ScaleDown:</p> <pre><code>Gemini (primary) \u2192 rate limited \u2192 ScaleDown compression-as-generation (fallback)\n</code></pre> <p>ScaleDown's internal model processes the system prompt + user question as \"context\" and produces a compressed, relevant extraction. While not a full generative response, it captures the key facts from the provided context.</p>"},{"location":"how-it-works/#role-of-google-gemini","title":"Role of Google Gemini","text":"<p>Gemini 2.5 Flash is the intelligence layer \u2014 it generates answers, classifies questions, extracts keywords, and powers the anti-hallucination pipeline.</p>"},{"location":"how-it-works/#gemini-usage-in-the-system","title":"Gemini Usage in the System","text":"Component Model Config Purpose Question Triage + Keywords <code>temperature=0.0</code> Classifies question complexity and extracts search keywords in one call COT Reasoning <code>max_tokens=8192</code>, <code>thinking_budget=2048</code> Full chain-of-thought analysis with citations Self-Verification <code>max_tokens=4096</code>, <code>thinking_budget=1024</code> Faster/cheaper \u2014 checks citations against source text Self-Critique <code>max_tokens=4096</code>, <code>thinking_budget=1024</code> Faster/cheaper \u2014 evaluates answer quality Direct Answers <code>max_tokens=8192</code>, <code>thinking_budget=2048</code> For general questions that don't need papers"},{"location":"how-it-works/#thinking-budget","title":"Thinking Budget","text":"<p>Gemini 2.5 Flash supports a <code>thinkingConfig</code> parameter that caps internal reasoning tokens. We use: - 2048 tokens for primary reasoning (COT, direct answers) - 1024 tokens for lighter tasks (verify, critique)</p> <p>This prevents the model from spending excessive time on internal reasoning, reducing latency by ~40%.</p>"},{"location":"how-it-works/#rate-limit-handling","title":"Rate Limit Handling","text":"<pre><code># Exponential backoff: 5s, 10s, 20s, 40s, 60s\n# After 5 retries \u2192 raises GeminiRateLimitError\n# Caller catches it \u2192 falls back to ScaleDown\n</code></pre>"},{"location":"how-it-works/#next-setup-instructions","title":"Next: Setup Instructions","text":"<p>See Getting Started for installation and configuration.</p>"},{"location":"improvements/","title":"Possible Improvements","text":"<p>Ideas for enhancing the system, organized by implementation complexity.</p>"},{"location":"improvements/#short-term","title":"Short-Term","text":"<p>These improvements can be implemented quickly with high impact.</p> Improvement Impact Effort Why It Matters Semantic embeddings (Sentence Transformers / Qwen3-Embedding) Much better retrieval quality Medium TF-IDF misses semantic similarity; embeddings capture meaning Cross-encoder re-ranking after initial TF-IDF retrieval Higher precision top-k Low Second-pass ranking eliminates noise from first-stage retrieval Async HTTP calls (aiohttp) for parallel Gemini + ScaleDown Lower latency Medium Currently sequential; parallel calls could save 50% time ScaleDown Python SDK (<code>pip install scaledown</code>) Cleaner code, batch support, built-in retry Low Raw HTTP calls are verbose; SDK handles boilerplate Response caching \u2014 cache Gemini responses by (question, context_hash) Eliminates repeat latency Low Same question on same papers = instant response Better PDF extraction \u2014 use <code>pymupdf</code> or <code>pdfplumber</code> Better text quality, especially tables Low PyPDF2 struggles with complex layouts; these libraries are more robust"},{"location":"improvements/#medium-term","title":"Medium-Term","text":"<p>These require more design work but have significant benefits.</p> Improvement Impact Effort Why It Matters ScaleDown SemanticOptimizer Replace TF-IDF entirely Medium Their FAISS-based semantic search is faster and more accurate than TF-IDF ScaleDown Pipeline \u2014 chain HasteOptimizer \u2192 Compressor Structured compression pipeline Medium Eliminates manual orchestration; built-in observability Multi-source support \u2014 Semantic Scholar API, PubMed, IEEE Xplore Much wider paper coverage High ArXiv-only limits domains (medicine, older CS papers, etc.) Streaming responses \u2014 stream Gemini output token-by-token Better UX for long answers Medium Users see progress instead of waiting 15s for full response Web UI (Streamlit/Gradio) Broader accessibility Medium CLI limits non-technical users; web UI is more approachable Configurable thinking budget per question complexity Better quality/speed trade-off Low General questions waste thinking budget; research questions need more Section-level citations \u2014 extract page/section from chunks More precise citations Medium \"arxiv:1706.03762 Section 3.2\" is better than just \"arxiv:1706.03762\""},{"location":"improvements/#long-term","title":"Long-Term","text":"<p>These are ambitious improvements requiring substantial effort.</p> Improvement Impact Effort Why It Matters ScaleDown Pareto Merging \u2014 dynamic model merging Potentially 30% cost reduction High Intelligently routes queries to optimal model in cost/quality trade-off space Knowledge graph \u2014 build citation graph across papers Deep cross-paper analysis High Multi-hop reasoning: \"How does paper A's findings relate to paper B's methods?\" Fine-tuned embeddings on scientific text Domain-specific retrieval High Sentence Transformers trained on arXiv abstracts would outperform general models Evaluation framework \u2014 automated hallucination detection Quantified quality metrics Medium ScaleDown's evaluation pipeline can score answer quality automatically Multi-agent architecture \u2014 separate agents for search, analysis, verification Better specialization High Single LLM does everything; specialist agents could improve quality Real-time monitoring \u2014 track latency, costs, cache hits System observability Medium Currently no metrics; hard to optimize without measurement Database backend \u2014 replace JSON files with SQLite/Postgres Multi-user, scalability Medium File-based storage doesn't scale; DB enables concurrent access"},{"location":"improvements/#retrieval-improvements","title":"Retrieval Improvements","text":""},{"location":"improvements/#semantic-embeddings","title":"Semantic Embeddings","text":"<p>Current: TF-IDF (keyword-based)</p> <p>Proposed: Sentence Transformers or Qwen3-Embedding</p> <p>Implementation: <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(chunks)\n\n# Then use FAISS or cosine similarity\n</code></pre></p> <p>Benefits: - Captures semantic meaning (not just keywords) - Handles synonyms, paraphrasing - Better cross-domain retrieval</p>"},{"location":"improvements/#cross-encoder-re-ranking","title":"Cross-Encoder Re-Ranking","text":"<p>Current: TF-IDF scores are final</p> <p>Proposed: Second-pass with cross-encoder</p> <p>Implementation: <pre><code>from sentence_transformers import CrossEncoder\n\nreranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\nscores = reranker.predict([(query, chunk) for chunk in top_k])\nreranked = [chunk for _, chunk in sorted(zip(scores, top_k), reverse=True)]\n</code></pre></p> <p>Benefits: - Higher precision (fewer irrelevant chunks) - Better top-5 than top-20 \u2192 top-5</p>"},{"location":"improvements/#scaledown-semanticoptimizer","title":"ScaleDown SemanticOptimizer","text":"<p>Current: Custom TF-IDF pipeline</p> <p>Proposed: Use ScaleDown's built-in semantic search</p> <p>Implementation: <pre><code>from scaledown import SemanticOptimizer\n\noptimizer = SemanticOptimizer(api_key=SCALEDOWN_API_KEY)\nresults = optimizer.query(\n    chunks=chunks,\n    query=user_question,\n    top_k=5\n)\n</code></pre></p> <p>Benefits: - FAISS-backed semantic search - Integrated with ScaleDown compression - No need to maintain custom retrieval code</p>"},{"location":"improvements/#compression-improvements","title":"Compression Improvements","text":""},{"location":"improvements/#scaledown-pipeline","title":"ScaleDown Pipeline","text":"<p>Current: Manual compression calls</p> <p>Proposed: ScaleDown <code>Pipeline</code> class</p> <p>Implementation: <pre><code>from scaledown import Pipeline, HasteOptimizer, Compressor\n\npipeline = Pipeline([\n    HasteOptimizer(rate=\"auto\"),\n    Compressor(model=\"gemini-2.5-flash\")\n])\n\nresult = pipeline.run(context=raw_text, prompt=user_question)\n</code></pre></p> <p>Benefits: - Structured, maintainable - Built-in retries and error handling - Observability (latency, compression stats)</p>"},{"location":"improvements/#multi-source-support","title":"Multi-Source Support","text":"<p>Current: ArXiv only</p> <p>Proposed: Add Semantic Scholar, PubMed, IEEE Xplore</p>"},{"location":"improvements/#semantic-scholar-api","title":"Semantic Scholar API","text":"<pre><code>import requests\n\nresponse = requests.get(\n    \"https://api.semanticscholar.org/graph/v1/paper/search\",\n    params={\"query\": user_query, \"limit\": 10}\n)\npapers = response.json()[\"data\"]\n</code></pre> <p>Benefits: - 200M+ papers across all domains - Free API with generous rate limits - Includes citations, references, metadata</p>"},{"location":"improvements/#pubmed-api","title":"PubMed API","text":"<pre><code>from Bio import Entrez\n\nEntrez.email = \"your_email@example.com\"\nhandle = Entrez.esearch(db=\"pubmed\", term=query, retmax=10)\nresults = Entrez.read(handle)\n</code></pre> <p>Benefits: - Medical and life sciences papers - High-quality, peer-reviewed</p>"},{"location":"improvements/#user-experience-improvements","title":"User Experience Improvements","text":""},{"location":"improvements/#streaming-responses","title":"Streaming Responses","text":"<p>Current: Wait for full response</p> <p>Proposed: Stream tokens as they're generated</p> <p>Implementation: <pre><code>for chunk in gemini_client.generate_stream(prompt):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Benefits: - Immediate feedback - Users can read while generation continues - Better perceived latency</p>"},{"location":"improvements/#web-ui","title":"Web UI","text":"<p>Current: CLI only</p> <p>Proposed: Streamlit or Gradio web app</p> <p>Streamlit Example: <pre><code>import streamlit as st\n\nquestion = st.text_input(\"Ask a research question\")\nif st.button(\"Ask\"):\n    with st.spinner(\"Researching...\"):\n        answer = ask_question(question)\n    st.markdown(answer)\n</code></pre></p> <p>Benefits: - No terminal knowledge required - Richer UI (charts, images, interactive tables) - Shareable URLs</p>"},{"location":"improvements/#cost-performance-improvements","title":"Cost &amp; Performance Improvements","text":""},{"location":"improvements/#response-caching","title":"Response Caching","text":"<p>Current: Every question hits API</p> <p>Proposed: Cache by (question, context_hash)</p> <p>Implementation: <pre><code>import hashlib\n\ncache = {}\nkey = f\"{question}:{hashlib.md5(context.encode()).hexdigest()}\"\n\nif key in cache:\n    return cache[key]\n\nresponse = gemini_client.generate(prompt)\ncache[key] = response\nreturn response\n</code></pre></p> <p>Benefits: - Instant responses for repeat questions - Lower API costs - Reduced rate limit pressure</p>"},{"location":"improvements/#async-parallelization","title":"Async Parallelization","text":"<p>Current: Sequential API calls</p> <p>Proposed: Parallel with asyncio</p> <p>Implementation: <pre><code>import asyncio\n\nasync def parallel_calls():\n    tasks = [\n        gemini_async.generate(prompt1),\n        scaledown_async.compress(context1),\n        scaledown_async.compress(context2)\n    ]\n    return await asyncio.gather(*tasks)\n</code></pre></p> <p>Benefits: - 50% latency reduction - Better resource utilization</p>"},{"location":"improvements/#scaledown-batching","title":"ScaleDown Batching","text":"<p>Current: One API call per compression</p> <p>Proposed: Batch multiple compressions</p> <p>Implementation: <pre><code>from scaledown import batch_compress\n\nresults = batch_compress([\n    {\"context\": chunk1, \"prompt\": query},\n    {\"context\": chunk2, \"prompt\": query},\n    {\"context\": chunk3, \"prompt\": query}\n])\n</code></pre></p> <p>Benefits: - Lower latency (1 network round-trip vs 3) - Potential cost savings</p>"},{"location":"improvements/#evaluation-monitoring","title":"Evaluation &amp; Monitoring","text":""},{"location":"improvements/#automated-hallucination-detection","title":"Automated Hallucination Detection","text":"<p>Current: Manual verification</p> <p>Proposed: ScaleDown evaluation pipeline</p> <p>Implementation: <pre><code>from scaledown import evaluate_response\n\nscore = evaluate_response(\n    question=user_question,\n    answer=cot_answer,\n    sources=retrieved_chunks\n)\n</code></pre></p> <p>Benefits: - Quantified quality metrics - Automatic flagging of low-confidence answers - A/B testing of prompts and models</p>"},{"location":"improvements/#system-metrics","title":"System Metrics","text":"<p>Proposed: Track latency, costs, cache hits</p> <p>Implementation: <pre><code>import time\n\nstart = time.time()\nresponse = gemini_client.generate(prompt)\nlatency = time.time() - start\n\nmetrics.log({\"latency\": latency, \"tokens\": len(response), \"cost\": calculate_cost(response)})\n</code></pre></p> <p>Benefits: - Identify bottlenecks - Optimize slow stages - Cost tracking per session</p>"},{"location":"improvements/#next-steps","title":"Next Steps","text":"<p>Priority order: 1. ScaleDown Python SDK (easiest, immediate code cleanup) 2. Better PDF extraction (<code>pymupdf</code> instead of PyPDF2) 3. Semantic embeddings (biggest retrieval quality gain) 4. Response caching (instant repeat queries) 5. Async parallelization (50% latency reduction)</p> <p>See Project Structure for where these changes would fit in the codebase.</p>"},{"location":"limitations/","title":"Limitations","text":"<p>Understanding the constraints and trade-offs of the current system.</p>"},{"location":"limitations/#arxiv-only-source","title":"ArXiv-Only Source","text":""},{"location":"limitations/#current-state","title":"Current State","text":"<ul> <li>Only ArXiv papers are supported as primary sources</li> <li>Cannot fetch papers from IEEE, ACM, Springer, PubMed, or other academic databases</li> <li>No access to commercial journals or paywalled content</li> </ul>"},{"location":"limitations/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Limited Coverage: Many important papers are not on ArXiv (especially older works, industry research, medical journals)</li> <li>Recency Bias: ArXiv focuses on preprints, which may not be peer-reviewed</li> <li>Domain Gaps: Medicine, biology, and some engineering fields have less ArXiv coverage</li> </ul>"},{"location":"limitations/#arxiv-api-constraints","title":"ArXiv API Constraints","text":"<ul> <li>Rate Limits: No authentication \u2192 basic rate limiting</li> <li>Keyword Search Only: Results sorted by basic keyword matching, not semantic relevance</li> <li>No Full-Text Search: Can only search titles, abstracts, authors, categories</li> <li>Metadata Only: API returns metadata; PDFs must be downloaded separately</li> </ul>"},{"location":"limitations/#pdf-extraction-quality","title":"PDF Extraction Quality","text":"<ul> <li>Heavily formatted papers: Tables, graphs, and complex layouts often extracted poorly by PyPDF2</li> <li>Mathematical notation: Equations frequently garbled or unreadable</li> <li>Figures: Images and diagrams completely lost</li> <li>Multi-column layouts: Column order sometimes mixed up</li> </ul>"},{"location":"limitations/#scaledown-api-constraints","title":"ScaleDown API Constraints","text":""},{"location":"limitations/#compression-only-service","title":"Compression-Only Service","text":"<ul> <li>ScaleDown is not an LLM \u2014 it cannot generate free-form answers</li> <li>The \"fallback generation\" is really compressed extraction, not true generation</li> <li>Cannot answer questions that require reasoning beyond the provided context</li> </ul>"},{"location":"limitations/#compression-quality","title":"Compression Quality","text":"<ul> <li>Very short texts (&lt;200 chars): Skipped, no compression applied</li> <li>Highly technical content: May lose nuance when compressed 40-60%</li> <li>Query dependency: Compression quality depends on how well the user question captures their intent</li> </ul>"},{"location":"limitations/#latency","title":"Latency","text":"<ul> <li>Each API call adds 1-3 seconds of latency</li> <li>Multiple compression calls (context + artifacts) \u2192 5-10s total</li> <li>No batching \u2192 sequential calls</li> </ul>"},{"location":"limitations/#cost","title":"Cost","text":"<ul> <li>Requires a valid API key \u2014 no free tier</li> <li>Usage-based pricing (per token compressed)</li> </ul>"},{"location":"limitations/#gemini-free-tier-limitations","title":"Gemini Free Tier Limitations","text":""},{"location":"limitations/#rate-limits","title":"Rate Limits","text":"<ul> <li>The free Gemini API has strict requests per minute and tokens per day limits</li> <li>Heavy usage triggers 429 errors</li> <li>Each question with full pipeline = 3-4 Gemini API calls (triage, COT, verify, critique)</li> </ul>"},{"location":"limitations/#model-capability","title":"Model Capability","text":"<ul> <li>Gemini 2.5 Flash: Fast but not as capable as Pro models for complex multi-hop reasoning</li> <li>Thinking Budget: The <code>thinkingConfig</code> parameter caps internal reasoning, potentially reducing quality on highly complex questions</li> <li>Citation Accuracy: Even with strict prompts, the model sometimes hallucinates citations or misattributes sources</li> </ul>"},{"location":"limitations/#context-window","title":"Context Window","text":"<ul> <li>While technically large (1M+ tokens), the effective context is limited by:</li> <li>Cost (more tokens = higher API cost)</li> <li>Quality degradation with very long contexts</li> <li>Latency (longer contexts \u2192 slower responses)</li> </ul>"},{"location":"limitations/#rag-limitations","title":"RAG Limitations","text":""},{"location":"limitations/#tf-idf-retrieval","title":"TF-IDF Retrieval","text":"<ul> <li>Keyword-based, not semantic</li> <li>Misses relevant chunks that use different terminology (synonym problem)</li> <li>Example: Query \"neural nets\" won't match \"artificial neural networks\" unless both terms appear</li> </ul>"},{"location":"limitations/#fixed-chunk-sizes","title":"Fixed Chunk Sizes","text":"<ul> <li>No respect for document structure \u2014 chunks may cut through:</li> <li>Sentences</li> <li>Paragraphs</li> <li>Tables</li> <li>Equations</li> <li>Section boundaries</li> <li>Context fragmentation can break semantic meaning</li> </ul>"},{"location":"limitations/#no-re-ranking","title":"No Re-Ranking","text":"<ul> <li>Retrieved chunks are scored solely by TF-IDF cosine similarity</li> <li>No cross-encoder or LLM-based re-ranking is applied</li> <li>First-stage retrieval is final \u2014 no second-pass refinement</li> </ul>"},{"location":"limitations/#source-tracking","title":"Source Tracking","text":"<ul> <li>Citations are at the paper level, not page/section level</li> <li>Example: <code>[arxiv:1706.03762]</code> \u2014 but which part of the paper?</li> <li>No automatic extraction of section/page metadata from chunks</li> </ul>"},{"location":"limitations/#general-limitations","title":"General Limitations","text":""},{"location":"limitations/#no-real-time-data","title":"No Real-Time Data","text":"<ul> <li>Only papers already on ArXiv</li> <li>No preprint servers (bioRxiv, medRxiv, SSRN, etc.)</li> <li>No blogs, conference talks, or live research</li> </ul>"},{"location":"limitations/#single-language","title":"Single Language","text":"<ul> <li>English papers only</li> <li>No multilingual support</li> <li>Papers in other languages will be extracted but likely produce poor results</li> </ul>"},{"location":"limitations/#no-figureimage-analysis","title":"No Figure/Image Analysis","text":"<ul> <li>Extracted text doesn't include figures or diagrams</li> <li>Cannot answer questions like \"What does Figure 3 show?\"</li> <li>No vision model integration</li> </ul>"},{"location":"limitations/#session-state","title":"Session State","text":"<ul> <li>Sessions stored as JSON files on disk, not in a database</li> <li>No multi-user support</li> <li>No cloud synchronization</li> <li>Sessions lost if files are deleted</li> </ul>"},{"location":"limitations/#no-evaluation-framework","title":"No Evaluation Framework","text":"<ul> <li>No automated hallucination detection</li> <li>No quantified quality metrics</li> <li>No benchmark datasets</li> <li>Manual verification required</li> </ul>"},{"location":"limitations/#performance-limitations","title":"Performance Limitations","text":""},{"location":"limitations/#latency_1","title":"Latency","text":"<p>Full pipeline with paper discovery: - Triage + keyword extraction: ~2s - ArXiv search + PDF download: ~10-20s (parallel) - Text extraction + chunking + indexing: ~5s - Retrieval + compression: ~3s - COT generation: ~10-15s - Verification: ~5-8s - Critique: ~5-8s - Total: ~45-65 seconds</p> <p>Direct answer (general question): - Triage: ~2s - Direct generation: ~5s - Total: ~7 seconds</p>"},{"location":"limitations/#throughput","title":"Throughput","text":"<ul> <li>Single-threaded execution (no parallel Gemini calls)</li> <li>No response streaming (wait for full response)</li> <li>Rate limits restrict concurrent users</li> </ul>"},{"location":"limitations/#security-privacy","title":"Security &amp; Privacy","text":""},{"location":"limitations/#api-keys-in-env","title":"API Keys in <code>.env</code>","text":"<ul> <li>Keys stored in plain text</li> <li>No encryption at rest</li> <li>Accidental git commits expose keys (mitigated by <code>.gitignore</code>)</li> </ul>"},{"location":"limitations/#no-authentication","title":"No Authentication","text":"<ul> <li>CLI tool has no user authentication</li> <li>Anyone with file system access can:</li> <li>View sessions</li> <li>Read artifacts</li> <li>Use your API keys</li> </ul>"},{"location":"limitations/#data-storage","title":"Data Storage","text":"<ul> <li>Papers, artifacts, sessions stored locally</li> <li>No data encryption</li> <li>No automatic cleanup of old data</li> </ul>"},{"location":"limitations/#next-possible-improvements","title":"Next: Possible Improvements","text":"<p>See Improvements for ideas to address these limitations.</p>"},{"location":"methodology/","title":"Methodology","text":"<p>This page explains the technical approaches and design patterns used in the Scientific Literature Explorer.</p>"},{"location":"methodology/#1-retrieval-augmented-generation-rag","title":"1. Retrieval-Augmented Generation (RAG)","text":"<pre><code>%%{init: {'theme':'base','themeVariables':{'fontFamily':'sans-serif'}}}%%\nflowchart LR\n    Paper([Paper Text]) --&gt; Chunk[Chunking&lt;br/&gt;1000 chars&lt;br/&gt;200 overlap]\n\n    Chunk --&gt; Vec[TF-IDF&lt;br/&gt;Vectorization&lt;br/&gt;sklearn]\n    Vec --&gt; Index[(Chunk Index&lt;br/&gt;Sparse Matrix)]\n\n    Query([User Query]) --&gt; QVec[Vectorize Query&lt;br/&gt;Same TF-IDF]\n    QVec --&gt; Sim[Cosine&lt;br/&gt;Similarity]\n    Index --&gt; Sim\n\n    Sim --&gt; TopK[Top-K Chunks&lt;br/&gt;default: 5]\n    TopK --&gt; Compress[ScaleDown&lt;br/&gt;Compression&lt;br/&gt;40-60% reduction]\n\n    Compress --&gt; LLM[Gemini&lt;br/&gt;Generation]\n    LLM --&gt; Answer([Answer with&lt;br/&gt;Citations])\n\n    style Paper fill:#424242,stroke:#212121,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Query fill:#424242,stroke:#212121,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Index fill:#1565c0,stroke:#0d47a1,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Compress fill:#f57c00,stroke:#e65100,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Answer fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Chunk fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Vec fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style QVec fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Sim fill:#6a1b9a,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n    style TopK fill:#6a1b9a,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n    style LLM fill:#1976d2,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:5,ry:5\n\n    linkStyle default stroke:#e0e0e0,stroke-width:2px</code></pre> <p>The RAG pattern ensures answers are grounded in actual paper content rather than relying solely on the LLM's training data:</p> <ul> <li>Chunking: Papers are split into overlapping segments (1000 chars, 200 overlap) to ensure no information is lost at boundaries</li> <li>TF-IDF Vectorization: scikit-learn's <code>TfidfVectorizer</code> creates sparse vector representations with English stop-word removal</li> <li>Cosine Similarity: Queries are matched against the chunk index; top-k (default 5) most similar chunks are retrieved</li> <li>Source Tracking: Every chunk retains its source label (e.g., <code>arxiv:2511.14362</code>) for citation tracing</li> </ul>"},{"location":"methodology/#2-context-compression-scaledown","title":"2. Context Compression (ScaleDown)","text":"<pre><code>%%{init: {'theme':'base','themeVariables':{'fontFamily':'sans-serif'}}}%%\nflowchart TD\n    Raw[Raw Retrieved Chunks&lt;br/&gt;1500 tokens&lt;br/&gt;Redundant verbose] --&gt; SD{ScaleDown API}\n\n    Query[User Question] --&gt; SD\n    SD --&gt; Analyze[Query-Aware&lt;br/&gt;Analysis]\n\n    Analyze --&gt; Remove[Remove Redundancy]\n    Remove --&gt; Preserve[Preserve Semantics]\n    Preserve --&gt; Optimize[Optimize for&lt;br/&gt;gemini-2.5-flash&lt;br/&gt;tokenizer]\n\n    Optimize --&gt; Compressed[Compressed Context&lt;br/&gt;600 tokens&lt;br/&gt;40-60% reduction]\n\n    Compressed --&gt; Benefits\n\n    subgraph Benefits [Benefits]\n        B1[Lower API Cost]\n        B2[Faster Response]\n        B3[More Context Fits&lt;br/&gt;in Window]\n        B4[Better Focus]\n    end\n\n    style Raw fill:#c62828,stroke:#b71c1c,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Query fill:#424242,stroke:#212121,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Compressed fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#fff,rx:5,ry:5\n    style SD fill:#f57c00,stroke:#e65100,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Analyze fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Remove fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Preserve fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Optimize fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Benefits fill:#6a1b9a,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n    style B1 fill:#1976d2,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:5,ry:5\n    style B2 fill:#1976d2,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:5,ry:5\n    style B3 fill:#1976d2,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:5,ry:5\n    style B4 fill:#1976d2,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:5,ry:5\n\n    linkStyle default stroke:#e0e0e0,stroke-width:2px</code></pre> <p>Raw retrieved chunks are often redundant. ScaleDown's compression: - Reduces token count by 40-60% while preserving semantics - Uses the user's question as a guide (<code>prompt</code> parameter) to prioritize relevant information - Optimizes for the target model's tokenizer (<code>gemini-2.5-flash</code>) - The <code>\"rate\": \"auto\"</code> setting lets ScaleDown determine optimal compression</p>"},{"location":"methodology/#3-multi-stage-reasoning-workflow","title":"3. Multi-Stage Reasoning Workflow","text":"<p>Inspired by research on self-verification and chain-of-verification (CoVe):</p> <ul> <li>Chain-of-Thought: Forces step-by-step reasoning, reducing reasoning errors</li> <li>Self-Verification: A separate LLM call cross-references every claim against source documents</li> <li>Self-Critique: An independent evaluator checks for completeness and accuracy</li> <li>Stages are configurable \u2014 enable, disable, or reorder via CLI</li> </ul>"},{"location":"methodology/#4-question-triage","title":"4. Question Triage","text":"<pre><code>%%{init: {'theme':'base','themeVariables':{'fontFamily':'sans-serif'}}}%%\nflowchart LR\n    Q([Question]) --&gt; Classify{Gemini Triage&lt;br/&gt;+ Keyword Extract}\n\n    Classify --&gt;|GENERAL&lt;br/&gt;What is CNN?| Direct[Direct Answer&lt;br/&gt;No Papers&lt;br/&gt;~5s]\n    Classify --&gt;|CONCEPTUAL&lt;br/&gt;Explain attention| Minimal[Basic Search&lt;br/&gt;Skip Critique&lt;br/&gt;~15s]\n    Classify --&gt;|RESEARCH&lt;br/&gt;Latest NAS methods?| Full[Full Discovery&lt;br/&gt;COT+Verify+Critique&lt;br/&gt;~45s]\n\n    Direct --&gt; Answer1([Answer])\n    Minimal --&gt; Papers1[Light Paper Fetch]\n    Papers1 --&gt; Workflow1[Workflow&lt;br/&gt;critique=OFF]\n    Workflow1 --&gt; Answer2([Answer])\n\n    Full --&gt; Papers2[Deep Paper Discovery]\n    Papers2 --&gt; Workflow2[Full Workflow&lt;br/&gt;All Stages ON]\n    Workflow2 --&gt; Answer3([Answer])\n\n    style Q fill:#424242,stroke:#212121,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Classify fill:#d32f2f,stroke:#b71c1c,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Direct fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Minimal fill:#f57c00,stroke:#e65100,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Full fill:#c62828,stroke:#b71c1c,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Answer1 fill:#1565c0,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:10,ry:10\n    style Answer2 fill:#1565c0,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:10,ry:10\n    style Answer3 fill:#1565c0,stroke:#0d47a1,stroke-width:2px,color:#fff,rx:10,ry:10\n    style Papers1 fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Papers2 fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Workflow1 fill:#6a1b9a,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Workflow2 fill:#6a1b9a,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n\n    linkStyle default stroke:#e0e0e0,stroke-width:2px</code></pre> <p>A single Gemini call classifies questions into three tiers: - General: Simple factual questions \u2192 answered directly (no paper fetch, ~5s) - Conceptual: Needs depth but not specific papers \u2192 uses workflow but may skip critique - Research: Needs actual papers \u2192 full discovery + workflow pipeline</p> <p>This saves 60-90 seconds for simple questions by skipping paper discovery entirely.</p>"},{"location":"methodology/#5-resilient-llm-strategy","title":"5. Resilient LLM Strategy","text":"<pre><code>%%{init: {'theme':'base','themeVariables':{'fontFamily':'sans-serif'}}}%%\nflowchart TD\n    Start([API Call]) --&gt; Gemini{Gemini API}\n\n    Gemini --&gt;|Success 200| Success([Return Response])\n    Gemini --&gt;|Rate Limit 429| Retry{Retry Count&lt;br/&gt;&lt; 5?}\n\n    Retry --&gt;|Yes| Wait[Exponential Backoff&lt;br/&gt;5s, 10s, 20s, 40s, 60s]\n    Wait --&gt; Gemini\n\n    Retry --&gt;|No, All Failed| Fallback[ScaleDown&lt;br/&gt;Compression-as-Generation]\n    Fallback --&gt; FallbackSuccess([Return Compressed&lt;br/&gt;Extraction])\n\n    Gemini --&gt;|Other Error| Fail([Raise Exception])\n\n    style Start fill:#424242,stroke:#212121,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Gemini fill:#1976d2,stroke:#0d47a1,stroke-width:3px,color:#fff,rx:5,ry:5\n    style Success fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#fff,rx:10,ry:10\n    style FallbackSuccess fill:#f57c00,stroke:#e65100,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Fail fill:#c62828,stroke:#b71c1c,stroke-width:3px,color:#fff,rx:10,ry:10\n    style Retry fill:#6a1b9a,stroke:#4a148c,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Wait fill:#00838f,stroke:#006064,stroke-width:2px,color:#fff,rx:5,ry:5\n    style Fallback fill:#ef6c00,stroke:#e65100,stroke-width:2px,color:#fff,rx:5,ry:5\n\n    linkStyle default stroke:#e0e0e0,stroke-width:2px</code></pre> <p>Implementation: <pre><code>Primary: Gemini 2.5 Flash (full generation)\n    \u2502\n    \u251c\u2500\u2500 Rate limited (429)?\n    \u2502   \u2514\u2500\u2500 Retry with exponential backoff (5\u00d7 up to 60s)\n    \u2502       \u2514\u2500\u2500 Still limited?\n    \u2502           \u2514\u2500\u2500 Fallback: ScaleDown compression-as-generation\n    \u2502\n    \u2514\u2500\u2500 Research Agent rate-limited?\n        \u2514\u2500\u2500 Heuristic keyword extraction (regex-based, no API call)\n</code></pre></p>"},{"location":"methodology/#next-anti-hallucination-details","title":"Next: Anti-Hallucination Details","text":"<p>See Anti-Hallucination Pipeline for the full verification workflow.</p>"},{"location":"project-structure/","title":"Project Structure","text":"<p>Complete overview of the codebase organization and file descriptions.</p>"},{"location":"project-structure/#directory-tree","title":"Directory Tree","text":"<pre><code>RAG/\n\u251c\u2500\u2500 .env                          # API keys (not in git)\n\u251c\u2500\u2500 .env.example                  # Template for API keys\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 Project.md                    # Original project specification\n\u251c\u2500\u2500 README.md                     # Main documentation\n\u251c\u2500\u2500 requirements.txt              # Python dependencies\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py                   # CLI entry point \u2014 all commands\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u251c\u2500\u2500 config.py             # Central configuration (env vars, paths)\n\u2502   \u2502   \u251c\u2500\u2500 gemini.py             # Gemini API client + rate-limit retry + fallback\n\u2502   \u2502   \u251c\u2500\u2500 llm.py                # LLM-agnostic handler factories (COT, verify, critique)\n\u2502   \u2502   \u251c\u2500\u2500 research_agent.py     # Smart discovery \u2014 triage, keywords, ArXiv, web fallback\n\u2502   \u2502   \u251c\u2500\u2500 scaledown.py          # ScaleDown compression client + generation fallback\n\u2502   \u2502   \u2514\u2500\u2500 session.py            # Session persistence (JSON-based conversation history)\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 papers/\n\u2502   \u2502   \u2514\u2500\u2500 fetcher.py            # ArXiv Atom API search + PDF download/extraction\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 rag/\n\u2502   \u2502   \u2514\u2500\u2500 pipeline.py           # TF-IDF chunking, retrieval, and ScaleDown compression\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 storage/\n\u2502   \u2502   \u2514\u2500\u2500 artifact_store.py     # Markdown artifact storage with compression\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 workflow/\n\u2502       \u2514\u2500\u2500 engine.py             # Configurable reasoning pipeline (stages, toggle, reorder)\n\u2502\n\u251c\u2500\u2500 artifacts/                    # Stored COT/verify/critique markdown files\n\u2502   \u251c\u2500\u2500 cot/\n\u2502   \u251c\u2500\u2500 self_verify/\n\u2502   \u2514\u2500\u2500 self_critique/\n\u2502\n\u251c\u2500\u2500 papers/                       # Cached PDFs and extracted text\n\u2502   \u251c\u2500\u2500 *.pdf\n\u2502   \u2514\u2500\u2500 *.txt\n\u2502\n\u251c\u2500\u2500 sessions/                     # Conversation history (JSON per session)\n\u2502   \u2514\u2500\u2500 *.json\n\u2502\n\u2514\u2500\u2500 docs/                         # MkDocs documentation\n    \u251c\u2500\u2500 index.md\n    \u251c\u2500\u2500 architecture.md\n    \u251c\u2500\u2500 how-it-works.md\n    \u251c\u2500\u2500 setup.md\n    \u251c\u2500\u2500 configuration.md\n    \u251c\u2500\u2500 usage.md\n    \u251c\u2500\u2500 methodology.md\n    \u251c\u2500\u2500 anti-hallucination.md\n    \u251c\u2500\u2500 api-reference.md\n    \u251c\u2500\u2500 project-structure.md\n    \u251c\u2500\u2500 limitations.md\n    \u2514\u2500\u2500 improvements.md\n</code></pre>"},{"location":"project-structure/#file-descriptions","title":"File Descriptions","text":""},{"location":"project-structure/#root-files","title":"Root Files","text":"File Description <code>.env</code> Contains API keys (not version controlled) <code>.env.example</code> Template for API keys with all configurable parameters <code>.gitignore</code> Excludes <code>.env</code>, <code>papers/</code>, <code>artifacts/</code>, <code>sessions/</code>, etc. <code>Project.md</code> Original project specification and requirements <code>README.md</code> Main documentation with architecture, usage, methodology <code>requirements.txt</code> Python package dependencies <code>mkdocs.yml</code> MkDocs configuration for documentation site"},{"location":"project-structure/#srcmainpy","title":"<code>src/main.py</code>","text":"<p>CLI entry point \u2014 defines all user-facing commands using Click:</p> <ul> <li><code>ask</code> \u2014 Research questions with auto-discovery</li> <li><code>papers</code> \u2014 Interactive paper explorer</li> <li><code>paper</code> \u2014 Deep-dive into specific paper</li> <li><code>search</code> \u2014 Quick ArXiv search</li> <li><code>sessions</code> \u2014 List conversation history</li> <li><code>workflow</code> \u2014 Configure reasoning pipeline</li> <li><code>artifacts</code> \u2014 List stored outputs</li> </ul>"},{"location":"project-structure/#srccore-core-logic","title":"<code>src/core/</code> \u2014 Core Logic","text":""},{"location":"project-structure/#configpy","title":"<code>config.py</code>","text":"<p>Central configuration manager: - Loads environment variables from <code>.env</code> - Defines default values (chunk size, top-k, timeouts) - Exports paths for <code>papers/</code>, <code>artifacts/</code>, <code>sessions/</code> - Validates required API keys</p>"},{"location":"project-structure/#geminipy","title":"<code>gemini.py</code>","text":"<p>Gemini API client with resilience: - <code>GeminiClient</code>: Wraps Google Generative AI SDK - <code>generate()</code>: Main generation with rate limit retry - Exponential backoff: 5s, 10s, 20s, 40s, 60s - <code>GeminiRateLimitError</code>: Raised after 5 failed retries - Supports <code>thinkingConfig</code> for thinking budget - Configurable <code>temperature</code> and <code>max_tokens</code></p>"},{"location":"project-structure/#llmpy","title":"<code>llm.py</code>","text":"<p>LLM-agnostic handler factories: - <code>create_cot_handler()</code>: Chain-of-thought with strict citations - <code>create_verify_handler()</code>: Citation verification - <code>create_critique_handler()</code>: Quality evaluation - <code>create_direct_handler()</code>: General question answering - Each handler encapsulates system prompts and generation config</p>"},{"location":"project-structure/#research_agentpy","title":"<code>research_agent.py</code>","text":"<p>Smart paper discovery: - <code>discover()</code>: Main entry point - Question classification: general/conceptual/research - Keyword extraction (Gemini or heuristic fallback) - ArXiv search integration - Parallel PDF download - Web search fallback (placeholder)</p>"},{"location":"project-structure/#scaledownpy","title":"<code>scaledown.py</code>","text":"<p>ScaleDown compression client: - <code>compress_context()</code>: Main compression endpoint - <code>compress_artifact()</code>: Compress reasoning outputs - <code>generate_compressed()</code>: Fallback generation when Gemini is rate-limited - Automatic retry on transient errors - Configurable timeout</p>"},{"location":"project-structure/#sessionpy","title":"<code>session.py</code>","text":"<p>Session persistence manager: - <code>Session</code>: Class representing a conversation - <code>save()</code>: Write session to JSON - <code>load()</code>: Read session from JSON - <code>add_turn()</code>: Append Q&amp;A pair - Tracks papers, artifacts, timestamps - Auto-creates <code>sessions/</code> directory</p>"},{"location":"project-structure/#srcpapers-paper-retrieval","title":"<code>src/papers/</code> \u2014 Paper Retrieval","text":""},{"location":"project-structure/#fetcherpy","title":"<code>fetcher.py</code>","text":"<p>ArXiv integration: - <code>PaperFetcher</code>: Main class - <code>search()</code>: Query ArXiv Atom API - <code>download_pdf()</code>: Fetch PDF from ArXiv - <code>extract_text()</code>: PyPDF2 extraction - Parallel downloads via <code>ThreadPoolExecutor</code> - Caching (checks <code>papers/</code> before downloading) - Auto-creates <code>papers/</code> directory</p>"},{"location":"project-structure/#srcrag-retrieval-compression","title":"<code>src/rag/</code> \u2014 Retrieval &amp; Compression","text":""},{"location":"project-structure/#pipelinepy","title":"<code>pipeline.py</code>","text":"<p>RAG pipeline implementation: - <code>RAGPipeline</code>: Main class - <code>chunk_text()</code>: Split text with overlap - <code>build_index()</code>: TF-IDF vectorization - <code>retrieve()</code>: Cosine similarity search - <code>compress_chunks()</code>: ScaleDown compression - Source tracking (every chunk labeled with <code>arxiv:ID</code>)</p>"},{"location":"project-structure/#srcstorage-artifact-management","title":"<code>src/storage/</code> \u2014 Artifact Management","text":""},{"location":"project-structure/#artifact_storepy","title":"<code>artifact_store.py</code>","text":"<p>Persistent storage for reasoning outputs: - <code>ArtifactStore</code>: Main class - <code>save()</code>: Write markdown artifact - <code>load()</code>: Read artifact - <code>compress()</code>: ScaleDown compression before storage - Metadata tracking (timestamps, token counts, compression stats) - Organized by stage: <code>cot/</code>, <code>self_verify/</code>, <code>self_critique/</code> - Auto-creates <code>artifacts/</code> directory</p>"},{"location":"project-structure/#srcworkflow-pipeline-orchestration","title":"<code>src/workflow/</code> \u2014 Pipeline Orchestration","text":""},{"location":"project-structure/#enginepy","title":"<code>engine.py</code>","text":"<p>Configurable reasoning pipeline: - <code>WorkflowEngine</code>: Main class - <code>execute()</code>: Run all enabled stages in order - <code>toggle_stage()</code>: Enable/disable stages - <code>reorder()</code>: Change execution order - Stages: COT, self_verify, self_critique - Config persistence (saved to <code>workflow.json</code>) - Rich progress display</p>"},{"location":"project-structure/#runtime-directories","title":"Runtime Directories","text":""},{"location":"project-structure/#papers","title":"<code>papers/</code>","text":"<p>Cached paper files: - <code>{arxiv_id}.pdf</code> \u2014 Downloaded PDFs - <code>{arxiv_id}.txt</code> \u2014 Extracted text</p> <p>Benefits: - No redundant downloads - Instant follow-up questions - Works offline (once papers are cached)</p>"},{"location":"project-structure/#artifacts","title":"<code>artifacts/</code>","text":"<p>Stored reasoning outputs organized by stage: - <code>cot/{id}.md</code> \u2014 Chain-of-thought analysis - <code>self_verify/{id}.md</code> \u2014 Citation verification tables - <code>self_critique/{id}.md</code> \u2014 Quality critiques</p> <p>Benefits: - Audit trail of reasoning - Debug verification failures - Reuse previous analyses</p>"},{"location":"project-structure/#sessions","title":"<code>sessions/</code>","text":"<p>Conversation history: - <code>{session_id}.json</code> \u2014 All Q&amp;A turns, papers, metadata</p> <p>Benefits: - Multi-turn conversations - Context across questions - Resume previous sessions</p>"},{"location":"project-structure/#next-limitations","title":"Next: Limitations","text":"<p>See Limitations for known constraints and trade-offs.</p>"},{"location":"setup/","title":"Getting Started","text":"<p>This guide walks you through setting up the Scientific Literature Explorer on your machine.</p>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>Before installing, ensure you have:</p> <ul> <li>Python \u2265 3.10 (3.11+ recommended)</li> <li>A ScaleDown API key \u2014 Get yours at ScaleDown Getting Started</li> <li>A Google Gemini API key \u2014 Free tier available at Google AI Studio</li> </ul>"},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone &lt;repo-url&gt;\ncd RAG\n</code></pre>"},{"location":"setup/#2-create-a-virtual-environment","title":"2. Create a Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"setup/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>Dependencies include: - <code>requests</code> \u2014 HTTP client for ScaleDown, Gemini, and ArXiv APIs - <code>python-dotenv</code> \u2014 Load <code>.env</code> configuration - <code>numpy</code> \u2014 Array operations for TF-IDF - <code>scikit-learn</code> \u2014 TF-IDF vectorizer and cosine similarity - <code>PyPDF2</code> \u2014 PDF text extraction from ArXiv papers - <code>rich</code> \u2014 Terminal UI (tables, panels, markdown, progress spinners)</p>"},{"location":"setup/#configuration","title":"Configuration","text":""},{"location":"setup/#1-create-env-file","title":"1. Create <code>.env</code> File","text":"<p>Copy the example environment file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"setup/#2-add-your-api-keys","title":"2. Add Your API Keys","text":"<p>Open <code>.env</code> in your favorite editor and fill in your keys:</p> <pre><code># Required\nSCALEDOWN_API_KEY=your_scaledown_api_key_here\nGEMINI_API_KEY=your_gemini_api_key_here\n\n# Optional Configuration Overrides\nSCALEDOWN_MODEL=gemini-2.5-flash    # Target model for compression optimization\nGEMINI_MODEL=gemini-2.5-flash       # Gemini model to use\nCHUNK_SIZE=1000                      # Characters per chunk\nCHUNK_OVERLAP=200                    # Overlap between chunks\nTOP_K=5                              # Number of chunks to retrieve\nSCALEDOWN_TIMEOUT=15                 # Timeout in seconds for ScaleDown API\n</code></pre> <p>See Configuration Reference for detailed explanations of each variable.</p>"},{"location":"setup/#directory-structure","title":"Directory Structure","text":"<p>The system will automatically create these folders on first use:</p> <pre><code>RAG/\n\u251c\u2500\u2500 papers/          # Downloaded PDFs and extracted text\n\u251c\u2500\u2500 artifacts/       # Stored reasoning outputs (COT, verify, critique)\n\u2502   \u251c\u2500\u2500 cot/\n\u2502   \u251c\u2500\u2500 self_verify/\n\u2502   \u2514\u2500\u2500 self_critique/\n\u2514\u2500\u2500 sessions/        # Conversation history (JSON per session)\n</code></pre>"},{"location":"setup/#verify-installation","title":"Verify Installation","text":"<p>Test that everything is working:</p> <pre><code># Ask a simple question\npython -m src.main ask \"What is a convolutional neural network?\"\n</code></pre> <p>If you see a response, you're ready to go! \ud83c\udf89</p>"},{"location":"setup/#next-usage-examples","title":"Next: Usage Examples","text":"<p>See Usage Guide for all available commands and examples.</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This page covers all CLI commands and common workflows.</p>"},{"location":"usage/#quick-reference","title":"Quick Reference","text":"Command Purpose <code>ask \"question\"</code> Research question \u2192 auto-discover papers \u2192 full pipeline <code>ask \"question\" --session ID</code> Continue an existing session <code>papers \"query\"</code> Interactive paper explorer \u2014 search, select, ask <code>paper &lt;arxiv_id&gt; \"question\"</code> Deep-dive into a specific paper <code>search \"query\"</code> Search ArXiv (listing only, no analysis) <code>sessions</code> List all conversation sessions <code>workflow show</code> Display current pipeline configuration <code>workflow toggle &lt;stage&gt; &lt;on\\|off&gt;</code> Enable/disable a workflow stage <code>workflow reorder &lt;s1,s2,...&gt;</code> Reorder workflow stages <code>artifacts list</code> List stored reasoning artifacts"},{"location":"usage/#command-details","title":"Command Details","text":""},{"location":"usage/#ask-research-questions","title":"<code>ask</code> \u2014 Research Questions","text":"<p>Ask a research question and let the system discover relevant papers:</p> <pre><code># Basic usage\npython -m src.main ask \"What are the latest advances in neural architecture search?\"\n\n# Continue a previous conversation\npython -m src.main ask \"How does this compare to random search?\" --session abc123\n</code></pre> <p>What happens: 1. Question is triaged (general/conceptual/research) 2. If research question: ArXiv papers discovered and downloaded 3. Papers chunked, indexed, and retrieved 4. Context compressed via ScaleDown 5. Full anti-hallucination pipeline runs (COT \u2192 Verify \u2192 Critique) 6. Answer displayed with citations 7. Session saved for follow-up questions</p> <p>Expected time: - General questions: ~5s (direct answer) - Research questions: ~30-60s (paper discovery + full pipeline)</p>"},{"location":"usage/#papers-interactive-paper-explorer","title":"<code>papers</code> \u2014 Interactive Paper Explorer","text":"<p>Search, browse, and analyze papers interactively:</p> <pre><code>python -m src.main papers \"attention mechanism transformers\"\n</code></pre> <p>Interactive Commands: - Type text: Ask a question about the currently selected paper - Type a number: Switch to a different paper from the list - <code>back</code>: Return to the paper list - <code>list</code>: Show the paper list again - <code>s</code>: Start a new search - <code>q</code>: Quit the explorer</p> <p>Features: - Persistent session: All interactions share the same session - No refetching: Papers are cached \u2014 follow-up questions are instant - Conversation history: LLM sees previous Q&amp;A for better context - Full pipeline: Each answer goes through COT \u2192 Verify</p> <p>Example session:</p> <pre><code>&gt; papers \"attention mechanism transformers\"\n[Paper list displayed]\n\nSelect a paper (1-10), or type 's' to search again, 'q' to quit: 1\n[Paper #1 selected: \"Attention Is All You Need\"]\n\nAsk a question (or 'back' to list, 'q' to quit): What is the multi-head attention mechanism?\n[Full pipeline runs \u2192 Answer displayed with citations]\n\nAsk a question (or 'back' to list, 'q' to quit): How does it differ from single-head attention?\n[Follow-up answered using session history]\n\nAsk a question (or 'back' to list, 'q' to quit): back\n[Returns to paper list]\n\nSelect a paper (1-10), or type 's' to search again, 'q' to quit: q\n</code></pre>"},{"location":"usage/#paper-deep-dive-into-a-specific-paper","title":"<code>paper</code> \u2014 Deep-Dive into a Specific Paper","text":"<p>Analyze a known ArXiv paper:</p> <pre><code>python -m src.main paper 1706.03762 \"What is the multi-head attention mechanism?\"\n</code></pre> <p>What happens: 1. Downloads ArXiv paper <code>1706.03762</code> (if not cached) 2. Extracts text and indexes it 3. Runs full pipeline with paper-specific grounding 4. Answer focused ONLY on content from this paper</p> <p>Paper-specific grounding means: - System prompt explicitly says \"analyze THIS specific paper\" - LLM instructed NOT to use training data - Must cite specific sections, equations, figures, or tables - If paper doesn't mention something, it must say so</p>"},{"location":"usage/#search-quick-arxiv-search","title":"<code>search</code> \u2014 Quick ArXiv Search","text":"<p>Search ArXiv without analysis:</p> <pre><code>python -m src.main search \"graph neural networks\"\n</code></pre> <p>Output: - Top 10 ArXiv results with titles, authors, and abstracts - No paper download or analysis - Useful for quickly finding relevant papers</p>"},{"location":"usage/#sessions-view-conversation-history","title":"<code>sessions</code> \u2014 View Conversation History","text":"<p>List all saved sessions:</p> <pre><code>python -m src.main sessions\n</code></pre> <p>Output: - Session IDs - Creation timestamps - Number of Q&amp;A turns - Papers involved</p> <p>Use a session ID with <code>ask --session</code> to continue a conversation.</p>"},{"location":"usage/#workflow-configure-pipeline","title":"<code>workflow</code> \u2014 Configure Pipeline","text":"<p>View and modify the reasoning pipeline:</p>"},{"location":"usage/#show-current-configuration","title":"Show Current Configuration","text":"<pre><code>python -m src.main workflow show\n</code></pre> <p>Output: - List of stages in order - Enabled/disabled status for each</p>"},{"location":"usage/#toggle-stages","title":"Toggle Stages","text":"<pre><code># Enable self-critique\npython -m src.main workflow toggle self_critique on\n\n# Disable self-verification (not recommended!)\npython -m src.main workflow toggle self_verify off\n</code></pre>"},{"location":"usage/#reorder-stages","title":"Reorder Stages","text":"<pre><code># Standard order\npython -m src.main workflow reorder cot,self_verify,self_critique\n\n# Custom order (e.g., critique before verify)\npython -m src.main workflow reorder cot,self_critique,self_verify\n</code></pre>"},{"location":"usage/#artifacts-view-stored-outputs","title":"<code>artifacts</code> \u2014 View Stored Outputs","text":"<p>List all stored reasoning artifacts:</p> <pre><code>python -m src.main artifacts list\n</code></pre> <p>Output: - Artifact IDs - Stage (cot, self_verify, self_critique) - Timestamps - File sizes</p> <p>Artifacts are stored in <code>artifacts/</code> as compressed markdown files.</p>"},{"location":"usage/#workflow-examples","title":"Workflow Examples","text":""},{"location":"usage/#example-1-quick-research","title":"Example 1: Quick Research","text":"<pre><code># Single command gets you a cited answer\npython -m src.main ask \"What are transformers in NLP?\"\n</code></pre> <p>Result: - ArXiv papers discovered - Relevant chunks retrieved and compressed - COT reasoning with citations - Verification of all citations - Critique of answer quality - Final answer displayed in terminal</p>"},{"location":"usage/#example-2-multi-turn-conversation","title":"Example 2: Multi-Turn Conversation","text":"<pre><code># First question\npython -m src.main ask \"What is neural architecture search?\"\n# Note the session ID in the output: abc123\n\n# Follow-up\npython -m src.main ask \"What are the main challenges?\" --session abc123\n\n# Another follow-up\npython -m src.main ask \"How does DARTS address these?\" --session abc123\n</code></pre> <p>Result: - Each follow-up has context from previous turns - Papers are cached (no re-downloading) - Session history grows with each interaction</p>"},{"location":"usage/#example-3-interactive-paper-exploration","title":"Example 3: Interactive Paper Exploration","text":"<pre><code>python -m src.main papers \"transformers\"\n\n# Select paper #1\n1\n\n# Ask questions\nWhat is the encoder structure?\nHow many layers does it have?\nWhat is positional encoding?\n\n# Switch to paper #3\n3\n\n# Compare\nHow does this differ from the original transformer?\n\n# Exit\nq\n</code></pre> <p>Result: - Seamless switching between papers - All questions share one session - Full pipeline runs for each answer</p>"},{"location":"usage/#example-4-custom-workflow","title":"Example 4: Custom Workflow","text":"<pre><code># Disable critique for faster responses\npython -m src.main workflow toggle self_critique off\n\n# Ask question (skips critique stage)\npython -m src.main ask \"What is gradient descent?\"\n\n# Re-enable critique\npython -m src.main workflow toggle self_critique on\n</code></pre> <p>Result: - Workflow configuration persists across runs - Faster responses when critique is disabled - Flexibility to trade quality for speed</p>"},{"location":"usage/#next-methodology","title":"Next: Methodology","text":"<p>See Methodology to understand the technical approach.</p>"}]}